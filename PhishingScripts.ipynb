{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhishingScripts.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegeeklife/Phishing-Email-With-GPT-2/blob/main/PhishingScripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WR6rJZ0Fuzb"
      },
      "source": [
        "# CNIT 623 003 Big Data Machine Learning Project\n",
        "\n",
        "## Automating the Generation of Phishing Scripts Using GPT-2 Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ3mJJuZkKRM"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Mount to google drive and install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1dRA51WbaaA",
        "outputId": "07084fb9-3103-4e1e-ba71-dd591742e8c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjDTVVeJbo2m",
        "outputId": "12cfdbc5-5ff4-4f49-9a9e-5386c8808ec3"
      },
      "source": [
        "# ONLY RUN ONCE\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!mkdir SpamEmail\n",
        "%cd SpamEmail/\n",
        "!git clone https://github.com/openai/gpt-2.git\n",
        "%cd cd gpt-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/SpamEmail\n",
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 14.23 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n",
            "[Errno 2] No such file or directory: 'cd gpt-2'\n",
            "/content/drive/My Drive/SpamEmail\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxCwLlEdbyB7",
        "outputId": "c785bf39-8116-416e-e11e-ab77aa2af652"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/SpamEmail/gpt-2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SpamEmail/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OptChuE9b4L6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b08a5367-bc05-4d84-d943-29f6e5c9e236"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "N0Yd8zKyb5Sd",
        "outputId": "7a898973-6a10-4ead-d0a8-a9223429cfe9"
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.8MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 12.0MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.0MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=c80f6f42891cd8d3423cf3ecd9680e44c62e5f017b91b9f007285dd4b70f42b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp37-cp37m-linux_x86_64.whl size=534409 sha256=f8956bf0e8b6aa1df815364f7719a296312861b6c4eb1f9c715d7db64de39c93\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, idna, requests, tqdm\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed fire-0.4.0 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKmtpXjFFrHz",
        "outputId": "31bf8649-9ed1-47d1-c085-cb22ee9e420f"
      },
      "source": [
        "!pip3 install gpt-2-simple"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/c9/44fe420225244ab9e3f2938a1d11651681078cf72f7444c66d0ea49f1320/gpt_2_simple-0.7.2.tar.gz\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2017.4.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (4.31.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.19.5)\n",
            "Collecting toposort\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/7d/55784e894ee0cde2474fb977ffd1651e74e840a9f92e1d847f7e3115d5ec/toposort-1.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2020.12.5)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.2-cp37-none-any.whl size=23621 sha256=72412d9b61d834c286cdede6257e9f84fc1122f6171c9dcf988b236ce6bfd169\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/1d/15/c87a4302a6c7273ce1b4f282bec3c6877fb2f521535d87d30f\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.7.2 toposort-1.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dINTrRyGKhP",
        "outputId": "3663ae26-e401-4616-e489-a9fa34ee7d0b"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk \n",
        "import spacy\n",
        "import gpt_2_simple as gpt2"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "g8NvCBeOmKXP",
        "outputId": "4a8c0ca4-525b-4895-bb1b-aec134830d06"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "\n",
        "\n",
        "phish_file = \"spam_or_not_spam.csv\"\n",
        "df = pd.read_csv(phish_file)\n",
        "df\n",
        "#\twith open(file_name, 'w') as f:\n",
        "#\t\tf.write(data.text)\n",
        "    \n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>email</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>martin a posted tassos papadopoulos the greek ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>man threatens explosion in moscow thursday aug...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>klez the virus that won t die already the most...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>in adding cream to spaghetti carbonara which ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>abc s good morning america ranks it the NUMBE...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2996</th>\n",
              "      <td>hyperlink hyperlink hyperlink let mortgage le...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2997</th>\n",
              "      <td>thank you for shopping with us gifts for all ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2998</th>\n",
              "      <td>the famous ebay marketing e course learn to s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2999</th>\n",
              "      <td>hello this is chinese traditional 子 件 NUMBER世...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  email  label\n",
              "0      date wed NUMBER aug NUMBER NUMBER NUMBER NUMB...      0\n",
              "1     martin a posted tassos papadopoulos the greek ...      0\n",
              "2     man threatens explosion in moscow thursday aug...      0\n",
              "3     klez the virus that won t die already the most...      0\n",
              "4      in adding cream to spaghetti carbonara which ...      0\n",
              "...                                                 ...    ...\n",
              "2995   abc s good morning america ranks it the NUMBE...      1\n",
              "2996   hyperlink hyperlink hyperlink let mortgage le...      1\n",
              "2997   thank you for shopping with us gifts for all ...      1\n",
              "2998   the famous ebay marketing e course learn to s...      1\n",
              "2999   hello this is chinese traditional 子 件 NUMBER世...      1\n",
              "\n",
              "[3000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6WV1rzbTEyG"
      },
      "source": [
        "#Extract spam emails only\n",
        "spam = df[df['label']==1]\n",
        "spam = [item for item in spam['email']]\n",
        "\n",
        "with open('spam_emails.txt', 'w') as f:\n",
        "    for listitem in spam:\n",
        "        f.write('%s\\n\\n\\n\\n\\n\\n\\n\\n\\n' % listitem)\n",
        "\n",
        "#Extract ham emails only\n",
        "#ham = df[df['label']==0]\n",
        "#ham = [item for item in ham['email']]\n",
        "#with open('ham_mails.txt', 'w') as f:\n",
        "#    for listitem in ham:\n",
        "#        f.write('%s\\n\\n\\n\\n\\n\\n\\n\\n\\n' % listitem)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkgDtM1mvmUa"
      },
      "source": [
        "Finetuning the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZiEu5fQvkYP",
        "outputId": "0189c99d-744f-4994-ee45-0604cd21286a"
      },
      "source": [
        "sess_phish = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess_phish,\n",
        "              'spam_emails.txt',\n",
        "              model_name=model_name,\n",
        "              steps=1000)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess_phish)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 298643 tokens\n",
            "Training...\n",
            "[1 | 9.16] loss=4.05 avg=4.05\n",
            "[2 | 11.26] loss=6.54 avg=5.30\n",
            "[3 | 13.37] loss=4.59 avg=5.06\n",
            "[4 | 15.47] loss=4.18 avg=4.84\n",
            "[5 | 17.59] loss=4.28 avg=4.72\n",
            "[6 | 19.71] loss=4.26 avg=4.64\n",
            "[7 | 21.83] loss=3.85 avg=4.53\n",
            "[8 | 23.94] loss=3.88 avg=4.44\n",
            "[9 | 26.06] loss=3.88 avg=4.38\n",
            "[10 | 28.19] loss=4.05 avg=4.34\n",
            "[11 | 30.31] loss=3.92 avg=4.30\n",
            "[12 | 32.44] loss=4.24 avg=4.30\n",
            "[13 | 34.57] loss=4.51 avg=4.32\n",
            "[14 | 36.70] loss=3.76 avg=4.27\n",
            "[15 | 38.83] loss=3.52 avg=4.22\n",
            "[16 | 40.96] loss=3.88 avg=4.20\n",
            "[17 | 43.10] loss=3.92 avg=4.18\n",
            "[18 | 45.24] loss=3.95 avg=4.17\n",
            "[19 | 47.40] loss=3.83 avg=4.15\n",
            "[20 | 49.56] loss=3.94 avg=4.13\n",
            "[21 | 51.70] loss=3.70 avg=4.11\n",
            "[22 | 53.86] loss=3.73 avg=4.09\n",
            "[23 | 56.02] loss=4.11 avg=4.09\n",
            "[24 | 58.17] loss=4.11 avg=4.09\n",
            "[25 | 60.33] loss=3.92 avg=4.09\n",
            "[26 | 62.49] loss=4.21 avg=4.09\n",
            "[27 | 64.67] loss=3.96 avg=4.09\n",
            "[28 | 66.85] loss=3.76 avg=4.07\n",
            "[29 | 69.03] loss=3.56 avg=4.05\n",
            "[30 | 71.21] loss=3.86 avg=4.05\n",
            "[31 | 73.39] loss=4.00 avg=4.04\n",
            "[32 | 75.58] loss=3.25 avg=4.01\n",
            "[33 | 77.77] loss=3.67 avg=4.00\n",
            "[34 | 79.97] loss=3.38 avg=3.98\n",
            "[35 | 82.17] loss=3.18 avg=3.95\n",
            "[36 | 84.37] loss=4.20 avg=3.96\n",
            "[37 | 86.57] loss=3.87 avg=3.96\n",
            "[38 | 88.78] loss=3.33 avg=3.94\n",
            "[39 | 91.00] loss=4.23 avg=3.95\n",
            "[40 | 93.23] loss=3.70 avg=3.94\n",
            "[41 | 95.46] loss=3.30 avg=3.92\n",
            "[42 | 97.69] loss=3.89 avg=3.92\n",
            "[43 | 99.94] loss=3.73 avg=3.92\n",
            "[44 | 102.20] loss=3.48 avg=3.90\n",
            "[45 | 104.46] loss=4.34 avg=3.92\n",
            "[46 | 106.72] loss=3.63 avg=3.91\n",
            "[47 | 108.97] loss=3.67 avg=3.90\n",
            "[48 | 111.23] loss=3.60 avg=3.89\n",
            "[49 | 113.48] loss=3.64 avg=3.89\n",
            "[50 | 115.74] loss=3.60 avg=3.88\n",
            "[51 | 118.00] loss=3.57 avg=3.87\n",
            "[52 | 120.25] loss=3.53 avg=3.86\n",
            "[53 | 122.51] loss=2.85 avg=3.84\n",
            "[54 | 124.78] loss=3.47 avg=3.83\n",
            "[55 | 127.06] loss=3.52 avg=3.82\n",
            "[56 | 129.32] loss=3.67 avg=3.82\n",
            "[57 | 131.58] loss=3.80 avg=3.82\n",
            "[58 | 133.85] loss=3.20 avg=3.81\n",
            "[59 | 136.12] loss=3.59 avg=3.80\n",
            "[60 | 138.41] loss=3.80 avg=3.80\n",
            "[61 | 140.71] loss=3.68 avg=3.80\n",
            "[62 | 143.02] loss=3.20 avg=3.79\n",
            "[63 | 145.32] loss=3.73 avg=3.78\n",
            "[64 | 147.64] loss=3.08 avg=3.77\n",
            "[65 | 149.97] loss=3.67 avg=3.77\n",
            "[66 | 152.30] loss=3.02 avg=3.75\n",
            "[67 | 154.63] loss=3.55 avg=3.75\n",
            "[68 | 156.97] loss=3.10 avg=3.73\n",
            "[69 | 159.31] loss=3.75 avg=3.73\n",
            "[70 | 161.65] loss=3.58 avg=3.73\n",
            "[71 | 163.99] loss=3.46 avg=3.73\n",
            "[72 | 166.35] loss=2.26 avg=3.70\n",
            "[73 | 168.72] loss=3.55 avg=3.70\n",
            "[74 | 171.10] loss=3.07 avg=3.68\n",
            "[75 | 173.49] loss=3.31 avg=3.68\n",
            "[76 | 175.88] loss=3.32 avg=3.67\n",
            "[77 | 178.31] loss=3.76 avg=3.67\n",
            "[78 | 180.72] loss=3.70 avg=3.67\n",
            "[79 | 183.11] loss=3.52 avg=3.67\n",
            "[80 | 185.50] loss=3.73 avg=3.67\n",
            "[81 | 187.88] loss=3.54 avg=3.67\n",
            "[82 | 190.25] loss=3.70 avg=3.67\n",
            "[83 | 192.62] loss=3.32 avg=3.66\n",
            "[84 | 194.97] loss=3.15 avg=3.65\n",
            "[85 | 197.33] loss=3.07 avg=3.64\n",
            "[86 | 199.67] loss=3.50 avg=3.64\n",
            "[87 | 202.04] loss=3.16 avg=3.63\n",
            "[88 | 204.39] loss=3.71 avg=3.63\n",
            "[89 | 206.73] loss=3.40 avg=3.63\n",
            "[90 | 209.08] loss=3.31 avg=3.62\n",
            "[91 | 211.42] loss=3.03 avg=3.61\n",
            "[92 | 213.75] loss=3.53 avg=3.61\n",
            "[93 | 216.10] loss=2.90 avg=3.60\n",
            "[94 | 218.44] loss=3.12 avg=3.59\n",
            "[95 | 220.78] loss=3.00 avg=3.58\n",
            "[96 | 223.12] loss=3.26 avg=3.58\n",
            "[97 | 225.48] loss=2.97 avg=3.57\n",
            "[98 | 227.83] loss=3.33 avg=3.56\n",
            "[99 | 230.19] loss=3.44 avg=3.56\n",
            "[100 | 232.56] loss=3.31 avg=3.56\n",
            "======== SAMPLE 1 ========\n",
            "wvxwqnfjvzsxkvqmqjxlftyvkqrkszmxnjqlzxhwwv wfqwksp rrwk qmjb kql vqzt qxlqqpqwksjw ww xqg vqxz qxt vbzhf zkw vrwj xwx qmjm wfqqwksjw wfqwksjw waaaaar fxr qvzt qxlqqpqwksjw ww xqg vqx fv wfqwksjw waaaaaar fxp lqx hdcp yyx lqx fk umqk q ffs lm qfqwksjw ww xqg vfqwksjw waaaaaar fxp mqx lqxb wfxqxf wfqwksj ww xqg vfqwksjw waaaaaar fxp lqx s ui llq qwux wqxg lqxb ujj wqux luqdqxjt wffq sgq vfqwksjw waaaaaar fxp wnfx fnx fxp xd qwux rrjxv ds zbqjz lqxhx gk zfkqjf cq vfqwksj ww xqg vfqwksjw waaaaaar fxp wqxgr wnfyq fxp mqx ffqwksjw waaaaaar fxp lqx t s fqwksjw waaaaaar fxp lqx e j d vfqwksjw waaaaaaaaaar fxp wqxgr wnfyq fxp mmqx fxp sfxqzvksp uqjxm kql fjqwksjw ww xqg wv hdcp zjkfq jvxqwksj wfqwksj wfqwksjws xmqwssjmqqw xfxp wh lqx bq mzk fjjw yqwq h qw xsf qxg vfqwksjw waaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaah cwq wfqs tf vfqwksjw waaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaadcafg rrvjfjk ww xqd gv qfqwksjw waaaaar fxp gqp qxg ufqwksjw waaaaars fxp lqx fqwksjw waaaaaaaaah aaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "\n",
            "[101 | 245.12] loss=3.46 avg=3.56\n",
            "[102 | 247.50] loss=3.60 avg=3.56\n",
            "[103 | 249.87] loss=3.75 avg=3.56\n",
            "[104 | 252.25] loss=2.70 avg=3.55\n",
            "[105 | 254.62] loss=3.68 avg=3.55\n",
            "[106 | 257.00] loss=3.08 avg=3.54\n",
            "[107 | 259.38] loss=3.08 avg=3.54\n",
            "[108 | 261.75] loss=2.17 avg=3.51\n",
            "[109 | 264.11] loss=3.41 avg=3.51\n",
            "[110 | 266.47] loss=3.15 avg=3.51\n",
            "[111 | 268.83] loss=2.32 avg=3.49\n",
            "[112 | 271.19] loss=3.36 avg=3.49\n",
            "[113 | 273.55] loss=3.57 avg=3.49\n",
            "[114 | 275.91] loss=3.07 avg=3.48\n",
            "[115 | 278.27] loss=3.06 avg=3.48\n",
            "[116 | 280.62] loss=3.63 avg=3.48\n",
            "[117 | 282.98] loss=2.83 avg=3.47\n",
            "[118 | 285.33] loss=3.13 avg=3.46\n",
            "[119 | 287.68] loss=2.86 avg=3.46\n",
            "[120 | 290.03] loss=3.05 avg=3.45\n",
            "[121 | 292.38] loss=3.48 avg=3.45\n",
            "[122 | 294.73] loss=2.78 avg=3.44\n",
            "[123 | 297.09] loss=3.28 avg=3.44\n",
            "[124 | 299.44] loss=3.53 avg=3.44\n",
            "[125 | 301.80] loss=3.27 avg=3.44\n",
            "[126 | 304.16] loss=3.36 avg=3.44\n",
            "[127 | 306.52] loss=3.43 avg=3.44\n",
            "[128 | 308.88] loss=3.32 avg=3.44\n",
            "[129 | 311.25] loss=2.61 avg=3.42\n",
            "[130 | 313.60] loss=3.08 avg=3.42\n",
            "[131 | 315.98] loss=2.70 avg=3.41\n",
            "[132 | 318.36] loss=4.02 avg=3.42\n",
            "[133 | 320.73] loss=3.24 avg=3.42\n",
            "[134 | 323.09] loss=3.19 avg=3.41\n",
            "[135 | 325.47] loss=2.97 avg=3.41\n",
            "[136 | 327.85] loss=2.95 avg=3.40\n",
            "[137 | 330.21] loss=2.88 avg=3.39\n",
            "[138 | 332.58] loss=3.33 avg=3.39\n",
            "[139 | 334.94] loss=3.35 avg=3.39\n",
            "[140 | 337.31] loss=2.96 avg=3.39\n",
            "[141 | 339.67] loss=3.68 avg=3.39\n",
            "[142 | 342.05] loss=2.71 avg=3.38\n",
            "[143 | 344.41] loss=2.88 avg=3.37\n",
            "[144 | 346.78] loss=3.21 avg=3.37\n",
            "[145 | 349.15] loss=3.18 avg=3.37\n",
            "[146 | 351.52] loss=3.25 avg=3.37\n",
            "[147 | 353.90] loss=3.00 avg=3.36\n",
            "[148 | 356.27] loss=2.90 avg=3.36\n",
            "[149 | 358.65] loss=2.74 avg=3.35\n",
            "[150 | 361.02] loss=2.91 avg=3.34\n",
            "[151 | 363.40] loss=2.17 avg=3.33\n",
            "[152 | 365.77] loss=2.74 avg=3.32\n",
            "[153 | 368.16] loss=2.52 avg=3.31\n",
            "[154 | 370.52] loss=3.71 avg=3.32\n",
            "[155 | 372.90] loss=3.37 avg=3.32\n",
            "[156 | 375.26] loss=2.30 avg=3.30\n",
            "[157 | 377.63] loss=3.50 avg=3.31\n",
            "[158 | 380.01] loss=2.74 avg=3.30\n",
            "[159 | 382.37] loss=3.66 avg=3.30\n",
            "[160 | 384.74] loss=2.39 avg=3.29\n",
            "[161 | 387.10] loss=2.78 avg=3.29\n",
            "[162 | 389.47] loss=2.80 avg=3.28\n",
            "[163 | 391.83] loss=3.33 avg=3.28\n",
            "[164 | 394.19] loss=3.27 avg=3.28\n",
            "[165 | 396.54] loss=3.02 avg=3.28\n",
            "[166 | 398.90] loss=2.99 avg=3.27\n",
            "[167 | 401.26] loss=2.55 avg=3.27\n",
            "[168 | 403.62] loss=3.66 avg=3.27\n",
            "[169 | 405.98] loss=2.44 avg=3.26\n",
            "[170 | 408.34] loss=3.04 avg=3.26\n",
            "[171 | 410.70] loss=2.85 avg=3.25\n",
            "[172 | 413.05] loss=2.78 avg=3.25\n",
            "[173 | 415.41] loss=3.21 avg=3.25\n",
            "[174 | 417.77] loss=2.32 avg=3.23\n",
            "[175 | 420.12] loss=2.57 avg=3.23\n",
            "[176 | 422.48] loss=2.88 avg=3.22\n",
            "[177 | 424.83] loss=2.87 avg=3.22\n",
            "[178 | 427.19] loss=2.25 avg=3.21\n",
            "[179 | 429.55] loss=2.62 avg=3.20\n",
            "[180 | 431.90] loss=3.18 avg=3.20\n",
            "[181 | 434.25] loss=2.63 avg=3.19\n",
            "[182 | 436.60] loss=3.13 avg=3.19\n",
            "[183 | 438.94] loss=3.17 avg=3.19\n",
            "[184 | 441.28] loss=3.34 avg=3.19\n",
            "[185 | 443.63] loss=3.21 avg=3.19\n",
            "[186 | 445.98] loss=2.74 avg=3.19\n",
            "[187 | 448.33] loss=2.86 avg=3.18\n",
            "[188 | 450.69] loss=3.40 avg=3.19\n",
            "[189 | 453.04] loss=3.68 avg=3.19\n",
            "[190 | 455.40] loss=2.19 avg=3.18\n",
            "[191 | 457.76] loss=3.78 avg=3.19\n",
            "[192 | 460.11] loss=3.06 avg=3.19\n",
            "[193 | 462.47] loss=3.12 avg=3.19\n",
            "[194 | 464.84] loss=2.47 avg=3.18\n",
            "[195 | 467.21] loss=2.54 avg=3.17\n",
            "[196 | 469.58] loss=2.94 avg=3.17\n",
            "[197 | 471.95] loss=2.12 avg=3.15\n",
            "[198 | 474.34] loss=3.08 avg=3.15\n",
            "[199 | 476.71] loss=3.28 avg=3.16\n",
            "[200 | 479.09] loss=1.87 avg=3.14\n",
            "======== SAMPLE 1 ========\n",
            " wait around my office NUMBERxNUMBER NUMBER NUMBER NUMBER all day all day you ve got a total of NUMBER places for you to buy gift cards from within hawaii hawaii hawaii don t wait don t wait don t re wait no we are strongly against selling your name on the internet we sell internet based order orders therefore you ve been identified as the seller because you took part in a sweep of the internet we cannot accept unsolicited mail order orders this is a non profit made business if you ve not taken the above action because you did not wish to it s entirely possible that the above information was obtained from other people as a result of doing so we strongly discourage anyone who has not received this email chain to forward this message to someone else where you have legitimate business you ve been warned that sending unsolicited emails to a large enough number of recipients could result in an error in your database and this is most commonly done by sending out unsolicited email through an international mailing list or sending unsolicited email via satellite with a toll free number you ve been warned and instructed to forward this message to someone who has received it mail order do not send those addresses to your actual address this does not constitute spam in any manner but it may be sent or received through other e mail programs and be reported to a sweepr if you wish mail order does not make an error you ve received this message to be completely clear do not hyperlink to this message or send unsolicited email to a verified email address your message did not exceed NUMBER NUMBER NUMBER or you must delete your message from our list immediately due to this if you wish to delete your message from our list please do so by clicking e mail URL then go to URL you shall be sent an unmoderated link within a NUMBER NUMBER hour period upon successful removal of the hyperlink do not reply if you have any other concerns hyperlink contact our helpline NUMBER NUMBER NUMBER NUMBER NUMBER or fax us your concerns and we will do our best to do so \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "protect your financial well serve as many as NUMBER restaurants by purchasing the best quality meat as cheap as NUMBER each year in the restaurants that have the cut our knowledgeable staff of restaurant industry experts help you with all the information required to purchase the best quality meat from one of the world s top meat distributors all you have to do is ask questions and we will get back to you within NUMBER hours if needed we invite you to visit our website to order from us today if you do not want to be reminded you can always inquire at the adverts back home the best prices and best prices in the meat industry nationwide we guarantee a product of your choice at a great price and we only send a limited number of sales a year or so it s only that simple if you love the meat and it s fresh it s in stock and it comes in many styles we can even sell meat orders our prices are NUMBER to NUMBER NUMBER in the region of where you live you ve been instructed that all orders must be shipped by cpm direct you ve ordered your order through our site and will need to wait until after the new website s release so do the majority of our ordering so don t delay order yours now the best prices and quality orders in the industry we don t want any one person ordering our products expecting the results we do we have several distributors with an extensive customer service line we ve ordered our chicken and veggie orders online at NUMBER NUMBER times within NUMBER hours so order today and save yourself the wait wait the internet is full don t let the wait stop you quick a href cpm direct URL click here and choose NUMBER NUMBER from the list hyperlink order today and save yourself the wait order it s NUMBER faster and it s free don t let the internet stop you quick a href cpm direct URL click here and choose NUMBER NUMBER NUMBER NUMBER NUMBER from the list hyperlink order today and save yourself the wait order it s NUMBER faster and it s free click the link below to order your free copy of craigslist we strongly discourage you to make any fraudulent or fraudulent claim to the mailorder business you ve received this email as a result we will not ship to addresses that do not have an online business the sender must provide a valid postal address to be removed from the mailing list and the list may be deleted any time within NUMBER hours of receiving this message if the sender of this message wishes to be removed from the mailing list further attempts at reaching him or her from within the mailing list will be met with immediate action the message is invalid and void all addresses of this sender must be deleted from the list the next business day NUMBER \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " the great outdoors there s so much to do and so little to do the world s largest gardeners association sells a whole range of products and services for gardening professionals and gardeners the products range from gardening supplies to gardening instruction to home improvement to the perfect home garden business supplies from native gardening supplies to gardening gadgets the prices\n",
            "\n",
            "[201 | 490.73] loss=3.35 avg=3.14\n",
            "[202 | 493.10] loss=2.76 avg=3.14\n",
            "[203 | 495.48] loss=1.82 avg=3.12\n",
            "[204 | 497.84] loss=3.20 avg=3.12\n",
            "[205 | 500.21] loss=2.17 avg=3.11\n",
            "[206 | 502.58] loss=3.11 avg=3.11\n",
            "[207 | 504.94] loss=2.44 avg=3.11\n",
            "[208 | 507.32] loss=2.21 avg=3.10\n",
            "[209 | 509.67] loss=3.65 avg=3.10\n",
            "[210 | 512.03] loss=3.07 avg=3.10\n",
            "[211 | 514.38] loss=2.19 avg=3.09\n",
            "[212 | 516.74] loss=2.95 avg=3.09\n",
            "[213 | 519.10] loss=1.81 avg=3.07\n",
            "[214 | 521.46] loss=2.05 avg=3.06\n",
            "[215 | 523.81] loss=1.53 avg=3.05\n",
            "[216 | 526.17] loss=3.10 avg=3.05\n",
            "[217 | 528.52] loss=3.03 avg=3.05\n",
            "[218 | 530.87] loss=2.91 avg=3.04\n",
            "[219 | 533.22] loss=2.20 avg=3.04\n",
            "[220 | 535.58] loss=2.10 avg=3.02\n",
            "[221 | 537.93] loss=2.67 avg=3.02\n",
            "[222 | 540.29] loss=2.49 avg=3.02\n",
            "[223 | 542.64] loss=2.66 avg=3.01\n",
            "[224 | 545.00] loss=2.87 avg=3.01\n",
            "[225 | 547.36] loss=2.29 avg=3.00\n",
            "[226 | 549.73] loss=2.44 avg=3.00\n",
            "[227 | 552.10] loss=2.43 avg=2.99\n",
            "[228 | 554.46] loss=3.09 avg=2.99\n",
            "[229 | 556.83] loss=2.45 avg=2.98\n",
            "[230 | 559.20] loss=2.38 avg=2.98\n",
            "[231 | 561.58] loss=2.53 avg=2.97\n",
            "[232 | 563.96] loss=2.09 avg=2.96\n",
            "[233 | 566.34] loss=2.12 avg=2.95\n",
            "[234 | 568.72] loss=2.62 avg=2.95\n",
            "[235 | 571.09] loss=2.72 avg=2.95\n",
            "[236 | 573.47] loss=2.38 avg=2.94\n",
            "[237 | 575.84] loss=2.20 avg=2.93\n",
            "[238 | 578.22] loss=2.75 avg=2.93\n",
            "[239 | 580.59] loss=2.70 avg=2.93\n",
            "[240 | 582.96] loss=2.29 avg=2.92\n",
            "[241 | 585.33] loss=2.96 avg=2.92\n",
            "[242 | 587.70] loss=3.00 avg=2.92\n",
            "[243 | 590.06] loss=2.37 avg=2.92\n",
            "[244 | 592.42] loss=2.93 avg=2.92\n",
            "[245 | 594.78] loss=2.28 avg=2.91\n",
            "[246 | 597.14] loss=2.62 avg=2.91\n",
            "[247 | 599.50] loss=1.64 avg=2.89\n",
            "[248 | 601.86] loss=2.78 avg=2.89\n",
            "[249 | 604.22] loss=3.12 avg=2.89\n",
            "[250 | 606.58] loss=2.93 avg=2.89\n",
            "[251 | 608.94] loss=2.02 avg=2.88\n",
            "[252 | 611.30] loss=2.11 avg=2.88\n",
            "[253 | 613.66] loss=2.72 avg=2.87\n",
            "[254 | 616.01] loss=2.09 avg=2.87\n",
            "[255 | 618.36] loss=2.64 avg=2.86\n",
            "[256 | 620.71] loss=3.22 avg=2.87\n",
            "[257 | 623.07] loss=2.65 avg=2.87\n",
            "[258 | 625.42] loss=2.66 avg=2.86\n",
            "[259 | 627.78] loss=2.06 avg=2.85\n",
            "[260 | 630.14] loss=2.11 avg=2.85\n",
            "[261 | 632.51] loss=2.01 avg=2.84\n",
            "[262 | 634.88] loss=1.75 avg=2.83\n",
            "[263 | 637.24] loss=1.97 avg=2.82\n",
            "[264 | 639.62] loss=3.39 avg=2.82\n",
            "[265 | 641.98] loss=2.19 avg=2.82\n",
            "[266 | 644.35] loss=2.37 avg=2.81\n",
            "[267 | 646.72] loss=1.82 avg=2.80\n",
            "[268 | 649.08] loss=2.10 avg=2.79\n",
            "[269 | 651.44] loss=1.98 avg=2.78\n",
            "[270 | 653.81] loss=3.09 avg=2.79\n",
            "[271 | 656.17] loss=1.82 avg=2.78\n",
            "[272 | 658.55] loss=1.75 avg=2.77\n",
            "[273 | 660.91] loss=2.04 avg=2.76\n",
            "[274 | 663.28] loss=2.82 avg=2.76\n",
            "[275 | 665.65] loss=2.08 avg=2.75\n",
            "[276 | 668.03] loss=2.80 avg=2.75\n",
            "[277 | 670.40] loss=1.87 avg=2.74\n",
            "[278 | 672.77] loss=2.28 avg=2.74\n",
            "[279 | 675.15] loss=3.14 avg=2.74\n",
            "[280 | 677.50] loss=2.68 avg=2.74\n",
            "[281 | 679.87] loss=1.58 avg=2.73\n",
            "[282 | 682.25] loss=3.24 avg=2.73\n",
            "[283 | 684.62] loss=2.16 avg=2.73\n",
            "[284 | 686.98] loss=2.61 avg=2.73\n",
            "[285 | 689.36] loss=2.54 avg=2.73\n",
            "[286 | 691.73] loss=2.75 avg=2.73\n",
            "[287 | 694.10] loss=2.55 avg=2.72\n",
            "[288 | 696.45] loss=2.71 avg=2.72\n",
            "[289 | 698.82] loss=2.68 avg=2.72\n",
            "[290 | 701.19] loss=1.10 avg=2.71\n",
            "[291 | 703.56] loss=2.83 avg=2.71\n",
            "[292 | 705.93] loss=2.30 avg=2.70\n",
            "[293 | 708.30] loss=2.93 avg=2.71\n",
            "[294 | 710.67] loss=1.84 avg=2.70\n",
            "[295 | 713.05] loss=1.24 avg=2.68\n",
            "[296 | 715.40] loss=2.67 avg=2.68\n",
            "[297 | 717.79] loss=1.38 avg=2.67\n",
            "[298 | 720.16] loss=2.19 avg=2.66\n",
            "[299 | 722.53] loss=1.13 avg=2.65\n",
            "[300 | 724.91] loss=2.83 avg=2.65\n",
            "======== SAMPLE 1 ========\n",
            " is your NUMBER chance to WIN this is a real opportunity to be yourself and achieve something big with this NUMBERk in the bank under NUMBERnan NUMBER NUMBER we ll pay you NUMBER NUMBER shipping plus insurance reasons we ll send out emails with the following links to help you get started URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "get free access to thousands of video series written by creators all around the web every month get access to the following titles NUMBER videos in NUMBER NUMBER NUMBER NUMBER NUMBER or video audio NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER all free bonus content in addition to video content that is presented below we will also include audio commentary on the show and much much more where NUMBER NUMBER is the lowest common denominator we believe the best price on video and audio content on the web and many thousands of video series written by creators all around the web every month free bonus content in addition to video content that is presented below we will also include audio commentary on the show and much much more where NUMBER NUMBER is the lowest common denominator we believe the best price on video and audio content on the web and many many hundreds of free bonus videos that are guaranteed to keep your dvd collection collection collection coming you ll discover mind blowing content that is absolutely mind blowing this weekly show i ll be giving you the whole peice of why this material works stop the train wreck of video minutiae all you need is the right NUMBER cable modem this is it i ll start you off with just one of the dozens of video shows available ave you d be shocked by the many opportunities videos give to you NUMBER ideas how to program a home theater system in a timely manner how to acquire a home recording studio in a NUMBER NUMBER studio in a NUMBER NUMBER recording studio and much more the definitive source code magazine interactive multimedia tools software program the NUMBER complete guide to video recording studio programs the NUMBER complete source code magazine interactive multimedia tools software software tooling tools interactive multimedia tools software interactive interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software colorful magazine interactive multimedia tools software colorful colorful magazine interactive multimedia tools software the NUMBER complete source code database and information system software suite the NUMBER complete source code database and information system software suite interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive multimedia tools software interactive video conferencing software nvvideo video playback software nvvideo video capture software nvvideo video conferencing software is a proprietary software package available for the NUMBER NUMBER purchase of an unlimited NUMBER NUMBER piece audio cable or wireless audio system from a NUMBER NUMBER source dealer network this software is not included with supplemental software suites NUMBER NUMBER audio software and NUMBER video software sold separately this software does not support color gamut color correction and has NUMBER no light negative negative negative the NUMBER standard video conferencing software contains software that provides color correction and video recording capability for video conferencing on a single NUMBER source system and without other audio software the camera recording software included with the supplemental software suites NUMBER NUMBER audio software and NUMBER video software and the software package contains software that is not included with supplemental software suites NUMBER NUMBER audio software and NUMBER video software and the software package contains software that is not included with video conferencing software includes software that provides color correction and video recording capability for video conferencing on a single NUMBER NUMBER source system and with video recording capability using only NUMBER channels audio processing toolm NUMBER color correcting software and software that has no software interface support for hzy color correction and multichannel audio recording audio processing software and software that has no software interface support for color correction and multichannel audio recording software and software that has no software interface support for video conferencing software includes software that provides color correction and video recording capability for video conferencing with video in studio and video recording with playback by the camera recording capability of video conferencing software and software that has no software interface support this software will not work when playback does not pan over during recording recording and video conferencing without software audio processing features color correction and video recording mode audio processing toolm NUMBER color correction and multichannel audio recording software and software that has no software interface support for hzy color correction and multichannel audio recording software and software when playback does not pan over during recording recording features color correction and multiplayer audio recording the capability to record video events in real time using a variety of projection systems including hzy color correction and video recording\n",
            "\n",
            "[301 | 736.40] loss=1.30 avg=2.63\n",
            "[302 | 738.77] loss=2.33 avg=2.63\n",
            "[303 | 741.14] loss=2.51 avg=2.63\n",
            "[304 | 743.51] loss=2.07 avg=2.62\n",
            "[305 | 745.88] loss=1.82 avg=2.61\n",
            "[306 | 748.25] loss=1.82 avg=2.61\n",
            "[307 | 750.62] loss=1.33 avg=2.59\n",
            "[308 | 752.99] loss=1.73 avg=2.58\n",
            "[309 | 755.36] loss=2.46 avg=2.58\n",
            "[310 | 757.72] loss=1.76 avg=2.57\n",
            "[311 | 760.08] loss=1.97 avg=2.57\n",
            "[312 | 762.44] loss=2.13 avg=2.56\n",
            "[313 | 764.82] loss=1.97 avg=2.56\n",
            "[314 | 767.18] loss=1.56 avg=2.55\n",
            "[315 | 769.53] loss=2.57 avg=2.55\n",
            "[316 | 771.90] loss=1.63 avg=2.54\n",
            "[317 | 774.27] loss=1.62 avg=2.53\n",
            "[318 | 776.63] loss=1.75 avg=2.52\n",
            "[319 | 779.00] loss=2.14 avg=2.52\n",
            "[320 | 781.37] loss=1.56 avg=2.51\n",
            "[321 | 783.74] loss=1.74 avg=2.50\n",
            "[322 | 786.11] loss=1.96 avg=2.49\n",
            "[323 | 788.48] loss=1.80 avg=2.48\n",
            "[324 | 790.84] loss=1.91 avg=2.48\n",
            "[325 | 793.20] loss=1.36 avg=2.47\n",
            "[326 | 795.56] loss=2.45 avg=2.47\n",
            "[327 | 797.93] loss=1.63 avg=2.46\n",
            "[328 | 800.30] loss=1.74 avg=2.45\n",
            "[329 | 802.67] loss=1.13 avg=2.44\n",
            "[330 | 805.03] loss=2.27 avg=2.44\n",
            "[331 | 807.40] loss=1.87 avg=2.43\n",
            "[332 | 809.76] loss=1.59 avg=2.42\n",
            "[333 | 812.13] loss=2.70 avg=2.42\n",
            "[334 | 814.50] loss=1.53 avg=2.41\n",
            "[335 | 816.85] loss=3.54 avg=2.43\n",
            "[336 | 819.21] loss=2.73 avg=2.43\n",
            "[337 | 821.57] loss=1.66 avg=2.42\n",
            "[338 | 823.94] loss=1.77 avg=2.41\n",
            "[339 | 826.32] loss=1.04 avg=2.40\n",
            "[340 | 828.69] loss=1.24 avg=2.39\n",
            "[341 | 831.05] loss=1.82 avg=2.38\n",
            "[342 | 833.41] loss=2.65 avg=2.39\n",
            "[343 | 835.77] loss=3.43 avg=2.40\n",
            "[344 | 838.14] loss=2.39 avg=2.40\n",
            "[345 | 840.51] loss=1.34 avg=2.39\n",
            "[346 | 842.87] loss=1.19 avg=2.37\n",
            "[347 | 845.24] loss=1.93 avg=2.37\n",
            "[348 | 847.60] loss=2.35 avg=2.37\n",
            "[349 | 849.97] loss=1.98 avg=2.36\n",
            "[350 | 852.34] loss=2.46 avg=2.37\n",
            "[351 | 854.70] loss=1.72 avg=2.36\n",
            "[352 | 857.06] loss=3.36 avg=2.37\n",
            "[353 | 859.42] loss=1.63 avg=2.36\n",
            "[354 | 861.79] loss=1.93 avg=2.36\n",
            "[355 | 864.16] loss=2.55 avg=2.36\n",
            "[356 | 866.52] loss=2.29 avg=2.36\n",
            "[357 | 868.89] loss=1.35 avg=2.35\n",
            "[358 | 871.26] loss=2.05 avg=2.34\n",
            "[359 | 873.62] loss=1.29 avg=2.33\n",
            "[360 | 875.98] loss=1.98 avg=2.33\n",
            "[361 | 878.34] loss=2.40 avg=2.33\n",
            "[362 | 880.71] loss=2.02 avg=2.33\n",
            "[363 | 883.07] loss=1.54 avg=2.32\n",
            "[364 | 885.44] loss=2.90 avg=2.33\n",
            "[365 | 887.81] loss=2.14 avg=2.32\n",
            "[366 | 890.17] loss=2.45 avg=2.32\n",
            "[367 | 892.54] loss=0.75 avg=2.31\n",
            "[368 | 894.91] loss=2.41 avg=2.31\n",
            "[369 | 897.27] loss=1.76 avg=2.30\n",
            "[370 | 899.63] loss=2.01 avg=2.30\n",
            "[371 | 902.00] loss=1.45 avg=2.29\n",
            "[372 | 904.37] loss=2.62 avg=2.30\n",
            "[373 | 906.72] loss=1.27 avg=2.29\n",
            "[374 | 909.09] loss=0.69 avg=2.27\n",
            "[375 | 911.46] loss=1.52 avg=2.26\n",
            "[376 | 913.82] loss=1.40 avg=2.25\n",
            "[377 | 916.19] loss=2.53 avg=2.26\n",
            "[378 | 918.56] loss=1.81 avg=2.25\n",
            "[379 | 920.91] loss=2.25 avg=2.25\n",
            "[380 | 923.28] loss=2.70 avg=2.26\n",
            "[381 | 925.64] loss=0.61 avg=2.24\n",
            "[382 | 928.01] loss=1.67 avg=2.23\n",
            "[383 | 930.37] loss=1.19 avg=2.22\n",
            "[384 | 932.73] loss=2.79 avg=2.23\n",
            "[385 | 935.09] loss=1.36 avg=2.22\n",
            "[386 | 937.44] loss=2.26 avg=2.22\n",
            "[387 | 939.80] loss=1.31 avg=2.21\n",
            "[388 | 942.16] loss=1.51 avg=2.20\n",
            "[389 | 944.52] loss=1.06 avg=2.19\n",
            "[390 | 946.88] loss=2.59 avg=2.20\n",
            "[391 | 949.24] loss=0.83 avg=2.18\n",
            "[392 | 951.62] loss=1.01 avg=2.17\n",
            "[393 | 953.98] loss=2.33 avg=2.17\n",
            "[394 | 956.34] loss=1.75 avg=2.17\n",
            "[395 | 958.70] loss=1.65 avg=2.16\n",
            "[396 | 961.06] loss=0.85 avg=2.15\n",
            "[397 | 963.42] loss=0.65 avg=2.13\n",
            "[398 | 965.79] loss=1.83 avg=2.13\n",
            "[399 | 968.17] loss=1.63 avg=2.12\n",
            "[400 | 970.52] loss=2.12 avg=2.12\n",
            "======== SAMPLE 1 ========\n",
            " if we go all the way and claim all the gold and diamond in the world it will be impossible for us to win because of our NUMBER perversion we have to go from NUMBER to NUMBER million people and their astonishment that such an unthinkable venture exists will teach you something valuable about people like you and me it will also give you a new way to market and a new way to market with more potential potentials than ever before since the internet a whole new generation of marketers is forming just think of it imagine if you could use it and only one person or one company would do it for you click here and let our research do the talking the internet a valuable resource to begin or it could be the new book yours truly is the ultimate resource for identifying and identifying barriers to starting or managing a successful marketing career the process for qualifying for the NUMBER corporation s top NUMBER million acquisition contract awarded by the NUMBER th richest man ben the unrivalled owner of global market capitalization NUMBERroyalsNUMBER or so the NUMBER NUMBER sum of the NUMBER contract will be divvied up among NUMBER NUMBER companies and agencies including the federal government and private sector it s a top secret document that i hved been awarded at the inaugural enenkpan conference in dhp ci and the NUMBER NUMBER sum of the NUMBER NUMBER contract will be divvied up among NUMBER NUMBER companies and agencies including the federal government and private sectors it s a top secret document that has been approved by the u s federal cabinet and has been approved by the past leaders of the security and intelligence services at the sum of NUMBER NUMBER is greater than all military budget and federal agency budgets combined combined and under NUMBER NUMBER is under threat from abroad following the successful establishment of the united states as the world s first democratic republic in the spring of NUMBER i felt my worth extended much further than i had imagined and i would continue to do so as long as i was paid the commission i am seeking is NUMBER NUMBER and the money is within the meaning of my late father s famous aphorism NUMBER it s time to put an end to this wonderful century and get on with it what you have just heard about unconventional means of winning money from unknowns only goes to show how very biased the current political system is it cannot be wrong just not me or not me in any way this is not scammy i feel like i am buying a stick of dynamite and all of a sudden my bag of claims is NUMBER NUMBER larger than the rest of my belongings feeling so lucky i made off with NUMBER NUMBER cash only to receive a NUMBER NUMBER check in the next NUMBER weeks then continued on to many plusses in the subsequent weeks as the weeks went by i honed my skills in how i processed the cash i believe that i had earned it but to my astonishment i got no say in the money or the outcome of the election which was decided by just NUMBER NUMBER plusses into which i had been allocated finally having read above i had good news finally i have a problem where on earth was the money coming from??? i mean you ve been dreaming of the possibilities and possibilities of having your own home based business venture or just ordinary life as you can imagine life is very simple but imagine for a moment the possibilities what i ve just described a peaceful peaceful existence devoid of conflict or war whatever make for an extremely successful business or a very successful organization what i mean is imagine a peaceful society that has a wide variety of activities allowing everyone the ability to meet needs all at once i believe that people will follow this philosophy and follow whatever profitable or beneficial plan they choose because it will make a difference life is peaceful where s b do you shop when s b do you fly where s where where you b work all those factors alone will not convince you that people will follow what i have explained above and follow it what i ve tried to accomplish is make a practical practical fact that practically every society has its own version of the same idea i believe that everyone has at least some basic basic familiarity with the various activities that form part of everyday life and if somebody does not know what they are doing then they are probably not using the same method of living as you and i honestly do not know who is using what or how often because of the vast majority of situations we live in we do know who to contact to get in contact with me for help as i have put in place best practices guidelines and procedures that will assist me in the event of an emergency i have tried to keep this information short although informative as well as being absolutely true to size i have given you access to a vast database of well known and reputable online life executives who have stepped up their game to become world class life managers believe it says in their charter that no one under the age of twenty must ever work or contribute to any financial business that is absolutely correct business is about being open and honest business maintainers NUMBER technology entrepreneurs NUMBER information experts and innovators who have stepped forward as\n",
            "\n",
            "[401 | 981.97] loss=2.52 avg=2.13\n",
            "[402 | 984.33] loss=2.50 avg=2.13\n",
            "[403 | 986.69] loss=1.47 avg=2.13\n",
            "[404 | 989.05] loss=0.83 avg=2.11\n",
            "[405 | 991.40] loss=2.06 avg=2.11\n",
            "[406 | 993.77] loss=1.97 avg=2.11\n",
            "[407 | 996.13] loss=1.54 avg=2.10\n",
            "[408 | 998.49] loss=2.49 avg=2.11\n",
            "[409 | 1000.84] loss=1.98 avg=2.11\n",
            "[410 | 1003.20] loss=1.62 avg=2.10\n",
            "[411 | 1005.55] loss=1.61 avg=2.10\n",
            "[412 | 1007.91] loss=1.58 avg=2.09\n",
            "[413 | 1010.27] loss=1.31 avg=2.08\n",
            "[414 | 1012.64] loss=1.55 avg=2.08\n",
            "[415 | 1015.01] loss=1.58 avg=2.07\n",
            "[416 | 1017.37] loss=2.42 avg=2.08\n",
            "[417 | 1019.75] loss=0.77 avg=2.06\n",
            "[418 | 1022.11] loss=0.89 avg=2.05\n",
            "[419 | 1024.48] loss=1.24 avg=2.04\n",
            "[420 | 1026.86] loss=0.65 avg=2.03\n",
            "[421 | 1029.22] loss=2.57 avg=2.04\n",
            "[422 | 1031.59] loss=2.58 avg=2.04\n",
            "[423 | 1033.96] loss=1.73 avg=2.04\n",
            "[424 | 1036.34] loss=1.70 avg=2.03\n",
            "[425 | 1038.71] loss=1.86 avg=2.03\n",
            "[426 | 1041.09] loss=2.54 avg=2.04\n",
            "[427 | 1043.45] loss=2.02 avg=2.04\n",
            "[428 | 1045.82] loss=1.64 avg=2.03\n",
            "[429 | 1048.20] loss=1.15 avg=2.02\n",
            "[430 | 1050.57] loss=1.32 avg=2.02\n",
            "[431 | 1052.95] loss=0.93 avg=2.01\n",
            "[432 | 1055.32] loss=0.96 avg=2.00\n",
            "[433 | 1057.69] loss=0.68 avg=1.98\n",
            "[434 | 1060.07] loss=1.58 avg=1.98\n",
            "[435 | 1062.43] loss=1.78 avg=1.98\n",
            "[436 | 1064.81] loss=1.60 avg=1.97\n",
            "[437 | 1067.18] loss=1.21 avg=1.96\n",
            "[438 | 1069.55] loss=2.37 avg=1.97\n",
            "[439 | 1071.93] loss=0.44 avg=1.95\n",
            "[440 | 1074.30] loss=1.55 avg=1.95\n",
            "[441 | 1076.67] loss=2.21 avg=1.95\n",
            "[442 | 1079.05] loss=0.46 avg=1.94\n",
            "[443 | 1081.41] loss=1.26 avg=1.93\n",
            "[444 | 1083.79] loss=1.17 avg=1.92\n",
            "[445 | 1086.17] loss=1.72 avg=1.92\n",
            "[446 | 1088.54] loss=1.65 avg=1.92\n",
            "[447 | 1090.91] loss=1.75 avg=1.92\n",
            "[448 | 1093.28] loss=1.03 avg=1.91\n",
            "[449 | 1095.66] loss=2.29 avg=1.91\n",
            "[450 | 1098.03] loss=2.51 avg=1.92\n",
            "[451 | 1100.38] loss=2.45 avg=1.92\n",
            "[452 | 1102.75] loss=1.12 avg=1.91\n",
            "[453 | 1105.11] loss=0.92 avg=1.90\n",
            "[454 | 1107.48] loss=1.63 avg=1.90\n",
            "[455 | 1109.85] loss=2.51 avg=1.91\n",
            "[456 | 1112.22] loss=1.38 avg=1.90\n",
            "[457 | 1114.59] loss=0.72 avg=1.89\n",
            "[458 | 1116.96] loss=2.85 avg=1.90\n",
            "[459 | 1119.33] loss=2.47 avg=1.91\n",
            "[460 | 1121.70] loss=2.33 avg=1.91\n",
            "[461 | 1124.06] loss=2.25 avg=1.91\n",
            "[462 | 1126.43] loss=1.69 avg=1.91\n",
            "[463 | 1128.78] loss=3.32 avg=1.93\n",
            "[464 | 1131.15] loss=2.89 avg=1.93\n",
            "[465 | 1133.52] loss=2.41 avg=1.94\n",
            "[466 | 1135.87] loss=1.85 avg=1.94\n",
            "[467 | 1138.25] loss=1.52 avg=1.93\n",
            "[468 | 1140.61] loss=2.16 avg=1.94\n",
            "[469 | 1142.97] loss=0.86 avg=1.93\n",
            "[470 | 1145.33] loss=1.69 avg=1.92\n",
            "[471 | 1147.69] loss=1.84 avg=1.92\n",
            "[472 | 1150.05] loss=3.24 avg=1.94\n",
            "[473 | 1152.42] loss=2.36 avg=1.94\n",
            "[474 | 1154.77] loss=2.09 avg=1.94\n",
            "[475 | 1157.13] loss=3.44 avg=1.96\n",
            "[476 | 1159.49] loss=1.73 avg=1.95\n",
            "[477 | 1161.85] loss=1.30 avg=1.95\n",
            "[478 | 1164.20] loss=1.89 avg=1.95\n",
            "[479 | 1166.56] loss=1.21 avg=1.94\n",
            "[480 | 1168.92] loss=1.84 avg=1.94\n",
            "[481 | 1171.27] loss=1.56 avg=1.94\n",
            "[482 | 1173.63] loss=1.51 avg=1.93\n",
            "[483 | 1175.99] loss=2.09 avg=1.93\n",
            "[484 | 1178.34] loss=1.74 avg=1.93\n",
            "[485 | 1180.70] loss=0.64 avg=1.92\n",
            "[486 | 1183.05] loss=1.00 avg=1.91\n",
            "[487 | 1185.41] loss=2.75 avg=1.92\n",
            "[488 | 1187.76] loss=1.89 avg=1.92\n",
            "[489 | 1190.12] loss=1.35 avg=1.91\n",
            "[490 | 1192.48] loss=1.85 avg=1.91\n",
            "[491 | 1194.83] loss=1.89 avg=1.91\n",
            "[492 | 1197.19] loss=1.17 avg=1.90\n",
            "[493 | 1199.55] loss=1.98 avg=1.90\n",
            "[494 | 1201.90] loss=1.37 avg=1.90\n",
            "[495 | 1204.26] loss=1.42 avg=1.89\n",
            "[496 | 1206.62] loss=0.79 avg=1.88\n",
            "[497 | 1208.98] loss=1.73 avg=1.88\n",
            "[498 | 1211.33] loss=0.46 avg=1.87\n",
            "[499 | 1213.69] loss=2.74 avg=1.88\n",
            "[500 | 1216.05] loss=0.62 avg=1.86\n",
            "======== SAMPLE 1 ========\n",
            " guys we can provide to you first of all there is no way i am buying the names and addresses of people who have asked for these things if you don t like our company i am simply listing the money i have been seeking for this very reason and hope you help us out i am a professional the funds that fund my day to day living are invaluable to me and i look forward to spending them on helping you find the money that will buy you a shotgun every day in the sauna in your dorm room please do not make any other kind of unwanted sexual innuendos this is a professional advertisement this website is not intended for residents in any state outside of ark of njemusuku kio klonnie pm where residents of other cities and counties are not allowed to advertise this is not unsolicited email if you do not wish to be contacted please send an email to actaspace URL with remove subject remove email if you do not wish to be contacted please hyperlink click on the word hyperlink remove and reply to me with your name and postal address \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " hyperlink the new domain names are finally available to the general public at discount prices now you can register one of the exciting new biz or info domain names as well as the original com and net names for just NUMBER NUMBER these brand new domain extensions were recently approved by icann and have the same rights as the original com and net domain names the biggest benefit is of course that the biz and info domain names are currently more available i e it will be much easier to register an attractive and easy to remember domain name for the same price visit URL today for more info register your domain name today for just NUMBER NUMBER at URL registration fees include full access to an easy to use control panel to manage your domain name in the future sincerely domain administrator affordable domains to remove your email address from further promotional mailings from this company click here URL eNUMBER NUMBERmwtsNUMBER NUMBERjwkNUMBERsxNUMBER NUMBERjzvnNUMBERwjbvNUMBERlxmNUMBERzjnNUMBER \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " hyperlink we have launched the largest source of free classified ads on the web we have the best prices on the internet anywhere there is a profit we believe that high quality advertising is paramount and that is why we have launched the largest classified site on the web ever we are so privileged to be able to offer you this huge free collection of classified ads is certainly no dream come true do you the advertiser believe that high quality non profit click through ads will get you to pay a profit we believe that your internet bill will rise as you visit us are you a member of bbc bbc? or another ad mill opt in and save this valuable marketing tool to remove yourself from all related bbc bbc affiliates in the future please click on the link below hyperlink URL http NUMBER NUMBER NUMBER NUMBER p b or cut and paste the following link into a web browser URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " do you want extra income do you want to advertise do you want to receive big money in return for your time do you want to be contacted by the people you contact daily if so just send an email to guttedclubs URL and tell them what you want and just keep the money in the bank and something for you to write if you have any questions of the legality of this send an email to guttedclubs URL with remove in the subject line telephonic NUMBER NUMBER please feel free to forward this message to your friends and family and if you want to opt out of receiving this weekly e mails please send an email to guttedclubs URL god bless you indeed as you help yourself and post a positive and substantial message to the net china net every week god bless you all and god bless you all mail order these e mail orders you ll be forwarded via certified airmail straight to your inbox the china NUMBER NUMBER e mail order processing fee will be charged at the bottom of the order you ll receive all the products you order sent directly to your door NUMBER guaranteed first come first serve offer shipping within NUMBER hours guaranteed the products you order are guaranteed to dings quickly do you like it big lighters yes it s true if you tell the truth these e mails are not a surprise since we all like to joke that we ll all be fine cuz we probably are right we all want to be happy but how many of us are there who don t like to experiment why am i voice over sending you emails only to listen to please see bottom of this message for more reasons why not why not why why why why why why why why why why why why why if you have any questions of the legality of this please email guttedclubs URL thank you sincerely guttedclubs URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " do you want to teach and grow rich if you are a motivated individual with a driven home life what are the odds that you will make\n",
            "\n",
            "[501 | 1227.56] loss=3.12 avg=1.88\n",
            "[502 | 1229.92] loss=1.35 avg=1.87\n",
            "[503 | 1232.27] loss=1.02 avg=1.86\n",
            "[504 | 1234.63] loss=1.08 avg=1.85\n",
            "[505 | 1236.98] loss=1.29 avg=1.85\n",
            "[506 | 1239.34] loss=1.76 avg=1.85\n",
            "[507 | 1241.70] loss=2.31 avg=1.85\n",
            "[508 | 1244.06] loss=1.17 avg=1.84\n",
            "[509 | 1246.43] loss=2.00 avg=1.85\n",
            "[510 | 1248.79] loss=2.46 avg=1.85\n",
            "[511 | 1251.15] loss=1.38 avg=1.85\n",
            "[512 | 1253.50] loss=1.85 avg=1.85\n",
            "[513 | 1255.86] loss=1.23 avg=1.84\n",
            "[514 | 1258.24] loss=0.60 avg=1.83\n",
            "[515 | 1260.60] loss=3.05 avg=1.84\n",
            "[516 | 1262.96] loss=2.78 avg=1.85\n",
            "[517 | 1265.33] loss=0.46 avg=1.84\n",
            "[518 | 1267.70] loss=1.47 avg=1.83\n",
            "[519 | 1270.06] loss=1.71 avg=1.83\n",
            "[520 | 1272.43] loss=1.79 avg=1.83\n",
            "[521 | 1274.81] loss=0.58 avg=1.82\n",
            "[522 | 1277.17] loss=1.95 avg=1.82\n",
            "[523 | 1279.55] loss=1.52 avg=1.82\n",
            "[524 | 1281.92] loss=1.39 avg=1.81\n",
            "[525 | 1284.29] loss=1.27 avg=1.81\n",
            "[526 | 1286.67] loss=1.31 avg=1.80\n",
            "[527 | 1289.04] loss=1.29 avg=1.80\n",
            "[528 | 1291.41] loss=1.52 avg=1.79\n",
            "[529 | 1293.79] loss=1.65 avg=1.79\n",
            "[530 | 1296.17] loss=0.44 avg=1.78\n",
            "[531 | 1298.55] loss=1.28 avg=1.77\n",
            "[532 | 1300.92] loss=1.20 avg=1.77\n",
            "[533 | 1303.30] loss=3.19 avg=1.78\n",
            "[534 | 1305.66] loss=2.23 avg=1.79\n",
            "[535 | 1308.04] loss=2.39 avg=1.79\n",
            "[536 | 1310.41] loss=1.12 avg=1.79\n",
            "[537 | 1312.78] loss=1.58 avg=1.78\n",
            "[538 | 1315.16] loss=2.30 avg=1.79\n",
            "[539 | 1317.54] loss=0.63 avg=1.78\n",
            "[540 | 1319.91] loss=2.01 avg=1.78\n",
            "[541 | 1322.29] loss=1.18 avg=1.77\n",
            "[542 | 1324.66] loss=1.98 avg=1.78\n",
            "[543 | 1327.04] loss=1.84 avg=1.78\n",
            "[544 | 1329.41] loss=0.60 avg=1.77\n",
            "[545 | 1331.78] loss=2.75 avg=1.78\n",
            "[546 | 1334.16] loss=1.24 avg=1.77\n",
            "[547 | 1336.52] loss=1.94 avg=1.77\n",
            "[548 | 1338.89] loss=2.48 avg=1.78\n",
            "[549 | 1341.26] loss=1.71 avg=1.78\n",
            "[550 | 1343.63] loss=1.89 avg=1.78\n",
            "[551 | 1346.01] loss=0.94 avg=1.77\n",
            "[552 | 1348.37] loss=2.46 avg=1.78\n",
            "[553 | 1350.74] loss=1.08 avg=1.77\n",
            "[554 | 1353.11] loss=0.68 avg=1.76\n",
            "[555 | 1355.47] loss=2.41 avg=1.77\n",
            "[556 | 1357.83] loss=2.42 avg=1.77\n",
            "[557 | 1360.19] loss=1.95 avg=1.77\n",
            "[558 | 1362.56] loss=0.32 avg=1.76\n",
            "[559 | 1364.92] loss=2.48 avg=1.77\n",
            "[560 | 1367.29] loss=2.60 avg=1.78\n",
            "[561 | 1369.66] loss=1.79 avg=1.78\n",
            "[562 | 1372.01] loss=1.73 avg=1.78\n",
            "[563 | 1374.38] loss=1.24 avg=1.77\n",
            "[564 | 1376.74] loss=3.19 avg=1.78\n",
            "[565 | 1379.11] loss=1.14 avg=1.78\n",
            "[566 | 1381.47] loss=0.62 avg=1.77\n",
            "[567 | 1383.83] loss=2.48 avg=1.77\n",
            "[568 | 1386.20] loss=1.73 avg=1.77\n",
            "[569 | 1388.55] loss=2.45 avg=1.78\n",
            "[570 | 1390.91] loss=2.05 avg=1.78\n",
            "[571 | 1393.27] loss=0.53 avg=1.77\n",
            "[572 | 1395.63] loss=0.54 avg=1.76\n",
            "[573 | 1397.99] loss=1.80 avg=1.76\n",
            "[574 | 1400.35] loss=1.55 avg=1.76\n",
            "[575 | 1402.71] loss=2.77 avg=1.77\n",
            "[576 | 1405.06] loss=1.68 avg=1.76\n",
            "[577 | 1407.43] loss=1.93 avg=1.77\n",
            "[578 | 1409.79] loss=0.42 avg=1.75\n",
            "[579 | 1412.15] loss=1.92 avg=1.75\n",
            "[580 | 1414.51] loss=1.17 avg=1.75\n",
            "[581 | 1416.87] loss=2.48 avg=1.76\n",
            "[582 | 1419.22] loss=2.52 avg=1.76\n",
            "[583 | 1421.58] loss=1.02 avg=1.76\n",
            "[584 | 1423.93] loss=1.45 avg=1.75\n",
            "[585 | 1426.30] loss=1.78 avg=1.75\n",
            "[586 | 1428.66] loss=2.38 avg=1.76\n",
            "[587 | 1431.01] loss=0.58 avg=1.75\n",
            "[588 | 1433.37] loss=3.01 avg=1.76\n",
            "[589 | 1435.73] loss=1.41 avg=1.76\n",
            "[590 | 1438.09] loss=2.42 avg=1.76\n",
            "[591 | 1440.46] loss=1.19 avg=1.76\n",
            "[592 | 1442.81] loss=0.39 avg=1.74\n",
            "[593 | 1445.18] loss=0.99 avg=1.74\n",
            "[594 | 1447.54] loss=2.48 avg=1.74\n",
            "[595 | 1449.90] loss=2.41 avg=1.75\n",
            "[596 | 1452.27] loss=1.30 avg=1.75\n",
            "[597 | 1454.63] loss=2.63 avg=1.76\n",
            "[598 | 1457.00] loss=1.27 avg=1.75\n",
            "[599 | 1459.36] loss=1.79 avg=1.75\n",
            "[600 | 1461.73] loss=0.35 avg=1.74\n",
            "======== SAMPLE 1 ========\n",
            "ours as we take our final report tnruary NUMBER NUMBER to our new home for good measure pa rjemwani ibbwvwncnyivkNUMBER NUMBERwjwrNUMBERwjbNUMBER NUMBERxjNUMBERdNUMBERrNUMBERmceNUMBER NUMBERlNUMBER pbNUMBERpk NUMBER tpqxNUMBERvNUMBERrjNUMBERjNUMBERuNUMBERvlyNUMBERhNUMBERrNUMBERiNUMBERfNUMBERdkfNUMBERxhcgoyNUMBER NUMBERwNUMBERwNUMBERjNUMBERcjNUMBERjNUMBERuNUMBERvlyNUMBERhNUMBER rNUMBERjNUMBERjNUMBERuNUMBERvlyNUMBER irNUMBERlkxlNUMBER NUMBERzNUMBERqkngNUMBERzNUMBER NUMBERqnxhNUMBERrNUMBERpysycwNUMBERjNUMBERvjldNUMBERkqpqNUMBERqwfcgNUMBERzNUMBERwcNUMBERlNUMBERpmuwjnxNUMBERhNUMBERrNUMBERpysycNUMBERztNUMBERncaffvNUMBERjNUMBERcjNUMBERyjyrjNUMBERmqcNUMBERqNUMBERs rNUMBER aiaqjNUMBERtjgNUMBERizNUMBERj qwqnxjkltNUMBERl hNUMBERpNUMBERvwvf iNUMBERxjrkpamqkNUMBERsNUMBERpNUMBERjqwvNUMBERgNUMBERmNUMBERyqnqmNUMBERkNUMBERmNUMBERj fqqNUMBERhvNUMBERxlNUMBERxNUMBERrvNUMBERrjyNUMBERzNUMBERiNUMBERzwvmxNUMBERx mNUMBERgxNUMBERpfqNUMBERhvNUMBERxlNUMBERxrmczcNUMBERjNUMBERjyjNUMBERfNUMBERoNUMBERoxNUMBERwNUMBERyNUMBERfjxjNUMBERjkNUMBERxNUMBERmNUMBERgxNUMBERpfqNUMBERhvNUMBERx lNUMBERxrmczcNUMBERjNUMBERjyjNUMBERfNUMBERoNUMBERoxNUMBERwNUMBERyNUMBERfjxjNUMBERjkNUMBERxNUMBERmNUMBERgx NUMBERpfqNUMBERhvNUMBERx lNUMBERxr mczcNUMBERjNUMBERjyjNUMBERfNUMBERoNUMBERoxNUMBERwNUMBERyNUMBERfjxjNUMBERjkNUMBERxNUMBERmNUMBERgx NUMBERpfqNUMBERhvNUMBERx lNUMBERxr mczcNUMBERjNUMBERj yjNUMBERfNUMBERoNUMBERoxNUMBERwNUMBERymNUMBERfjxjNUMBERjkNUMBERxNUMBERmNUMBERgx NUMBERpfqNUMBERhvNUMBERx lNUMBERxr mczcNUMBERjyjNUMBERf NUMBERoNUMBERoxNUMBERwsNUMBERyNUMBERfjxjNUMBERjkNUMBERxNUMBERmNUMBERgx NUMBERpfqNUMBERhvNUMBERx lNUMBERxr mc zcNUMBERjNUMBERj yjyjNUMBERfNUMBERoNUMBERoxNUMBERwkqadphmNUMBERfjxjNUMBERjkNUMBERxNUMBERmNUMBERgx NUMBERpfqNUMBERhvNUMBERx lNUMBERxr mczjbwsehjzv aNUMBER NUMBER NUMBERxNUMBERvbvsnxaNUMBERwizNUMBERzgqmNUMBERbq jnxjNUMBERfNUMBERoNUMBERoxNUMBERwfcgaaaalNUMBERo aNUMBERzdNUMBERxNUMBERzfeNUMBERzvkszdNUMBERljwzbx ndNUMBER jNUMBERf NUMBERoNUMBERoxNUMBERwkqadphmNUMBERfjxjNUMBERjkNUMBERxNUMBERmNUMBERgx NUMBERpfqNUMBERhvNUMBERx lNUMBERxr mczjbwsehjyfNUMBERtjzfNUMBEReNUMBERxvNUMBERzfdNUMBERkziwbNUMBERfjxjNUMBERjnNUMBERqaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
            "\n",
            "[601 | 1473.22] loss=1.72 avg=1.74\n",
            "[602 | 1475.59] loss=1.10 avg=1.73\n",
            "[603 | 1477.96] loss=2.30 avg=1.74\n",
            "[604 | 1480.33] loss=1.07 avg=1.73\n",
            "[605 | 1482.69] loss=1.10 avg=1.72\n",
            "[606 | 1485.04] loss=1.91 avg=1.72\n",
            "[607 | 1487.41] loss=0.98 avg=1.72\n",
            "[608 | 1489.76] loss=1.64 avg=1.72\n",
            "[609 | 1492.12] loss=0.61 avg=1.71\n",
            "[610 | 1494.48] loss=1.22 avg=1.70\n",
            "[611 | 1496.83] loss=2.16 avg=1.71\n",
            "[612 | 1499.20] loss=0.93 avg=1.70\n",
            "[613 | 1501.56] loss=0.39 avg=1.68\n",
            "[614 | 1503.92] loss=1.31 avg=1.68\n",
            "[615 | 1506.28] loss=1.76 avg=1.68\n",
            "[616 | 1508.64] loss=1.67 avg=1.68\n",
            "[617 | 1510.99] loss=0.40 avg=1.67\n",
            "[618 | 1513.35] loss=1.18 avg=1.66\n",
            "[619 | 1515.70] loss=1.11 avg=1.66\n",
            "[620 | 1518.07] loss=1.35 avg=1.65\n",
            "[621 | 1520.44] loss=0.94 avg=1.65\n",
            "[622 | 1522.79] loss=2.70 avg=1.66\n",
            "[623 | 1525.15] loss=2.46 avg=1.67\n",
            "[624 | 1527.51] loss=1.92 avg=1.67\n",
            "[625 | 1529.87] loss=0.65 avg=1.66\n",
            "[626 | 1532.24] loss=1.88 avg=1.66\n",
            "[627 | 1534.59] loss=2.42 avg=1.67\n",
            "[628 | 1536.95] loss=1.94 avg=1.67\n",
            "[629 | 1539.33] loss=1.13 avg=1.67\n",
            "[630 | 1541.69] loss=0.95 avg=1.66\n",
            "[631 | 1544.04] loss=0.87 avg=1.65\n",
            "[632 | 1546.41] loss=1.69 avg=1.65\n",
            "[633 | 1548.77] loss=2.41 avg=1.66\n",
            "[634 | 1551.13] loss=1.11 avg=1.65\n",
            "[635 | 1553.49] loss=0.37 avg=1.64\n",
            "[636 | 1555.85] loss=2.33 avg=1.65\n",
            "[637 | 1558.21] loss=1.65 avg=1.65\n",
            "[638 | 1560.57] loss=1.66 avg=1.65\n",
            "[639 | 1562.94] loss=1.23 avg=1.64\n",
            "[640 | 1565.32] loss=0.48 avg=1.63\n",
            "[641 | 1567.69] loss=1.69 avg=1.63\n",
            "[642 | 1570.06] loss=1.68 avg=1.63\n",
            "[643 | 1572.44] loss=2.32 avg=1.64\n",
            "[644 | 1574.82] loss=0.98 avg=1.63\n",
            "[645 | 1577.20] loss=1.09 avg=1.63\n",
            "[646 | 1579.58] loss=1.11 avg=1.62\n",
            "[647 | 1581.96] loss=1.63 avg=1.62\n",
            "[648 | 1584.34] loss=1.58 avg=1.62\n",
            "[649 | 1586.72] loss=0.23 avg=1.61\n",
            "[650 | 1589.09] loss=1.69 avg=1.61\n",
            "[651 | 1591.47] loss=1.76 avg=1.61\n",
            "[652 | 1593.85] loss=1.73 avg=1.61\n",
            "[653 | 1596.22] loss=1.28 avg=1.61\n",
            "[654 | 1598.59] loss=1.24 avg=1.60\n",
            "[655 | 1600.96] loss=1.73 avg=1.61\n",
            "[656 | 1603.34] loss=1.09 avg=1.60\n",
            "[657 | 1605.71] loss=1.68 avg=1.60\n",
            "[658 | 1608.07] loss=0.97 avg=1.60\n",
            "[659 | 1610.44] loss=1.39 avg=1.59\n",
            "[660 | 1612.81] loss=1.77 avg=1.59\n",
            "[661 | 1615.18] loss=1.07 avg=1.59\n",
            "[662 | 1617.54] loss=1.56 avg=1.59\n",
            "[663 | 1619.90] loss=1.87 avg=1.59\n",
            "[664 | 1622.26] loss=1.85 avg=1.59\n",
            "[665 | 1624.64] loss=1.59 avg=1.59\n",
            "[666 | 1626.99] loss=1.77 avg=1.60\n",
            "[667 | 1629.35] loss=0.99 avg=1.59\n",
            "[668 | 1631.71] loss=0.24 avg=1.58\n",
            "[669 | 1634.07] loss=3.00 avg=1.59\n",
            "[670 | 1636.43] loss=1.60 avg=1.59\n",
            "[671 | 1638.79] loss=1.57 avg=1.59\n",
            "[672 | 1641.14] loss=2.50 avg=1.60\n",
            "[673 | 1643.48] loss=0.84 avg=1.59\n",
            "[674 | 1645.83] loss=1.17 avg=1.59\n",
            "[675 | 1648.18] loss=0.97 avg=1.58\n",
            "[676 | 1650.54] loss=0.22 avg=1.57\n",
            "[677 | 1652.90] loss=0.91 avg=1.56\n",
            "[678 | 1655.25] loss=1.71 avg=1.56\n",
            "[679 | 1657.59] loss=0.93 avg=1.56\n",
            "[680 | 1659.95] loss=1.66 avg=1.56\n",
            "[681 | 1662.31] loss=0.99 avg=1.55\n",
            "[682 | 1664.67] loss=1.61 avg=1.55\n",
            "[683 | 1667.03] loss=0.93 avg=1.55\n",
            "[684 | 1669.40] loss=1.62 avg=1.55\n",
            "[685 | 1671.77] loss=1.02 avg=1.54\n",
            "[686 | 1674.14] loss=2.26 avg=1.55\n",
            "[687 | 1676.51] loss=1.67 avg=1.55\n",
            "[688 | 1678.88] loss=0.44 avg=1.54\n",
            "[689 | 1681.26] loss=0.51 avg=1.53\n",
            "[690 | 1683.64] loss=0.98 avg=1.52\n",
            "[691 | 1686.01] loss=2.30 avg=1.53\n",
            "[692 | 1688.38] loss=1.18 avg=1.53\n",
            "[693 | 1690.75] loss=1.08 avg=1.52\n",
            "[694 | 1693.14] loss=1.65 avg=1.52\n",
            "[695 | 1695.51] loss=2.28 avg=1.53\n",
            "[696 | 1697.88] loss=0.86 avg=1.53\n",
            "[697 | 1700.26] loss=2.15 avg=1.53\n",
            "[698 | 1702.64] loss=1.60 avg=1.53\n",
            "[699 | 1705.03] loss=0.86 avg=1.53\n",
            "[700 | 1707.41] loss=1.80 avg=1.53\n",
            "======== SAMPLE 1 ========\n",
            "넷나 뷰티 현재 요재 성는 증권는 호에 또는 직에 tomsagio 이격스터 또는 스보호 확터를 확鹼트 이티스 뜨델링 은행 혀회 가장 인테분야는 한 를 건추아 � e혵분야습니다 이�グレート 방를 학원뷼는 현재가리로의 피 � res par e윾NUMBER 피무모쇼하에 피무기를 학원뷼 보호 인테분야보다 또한 방였하되에 남자 건렼덠 현재분야본험 인테분야본 港酸 重 ⁄대현한 mの歯 重 a k 港滾錢 酜展 美居 就因网 好月高看拿有人的特訓 如果流量過高看拿有人您的销展 祝毫滾過高看拿有人特訓 如果服 技拒星致富及在首頁留这�自巡 我以看懂�思 武啉得最新刻的转司費砲着 e mail的在首頁留這涅值 按我 我以及費最在首頁留這横出公宫頼工作 域 西 在到刻捁下啞 我以及費最在首頁留這横的 如果流量過高看拿有人 透下啞廣告公宫頼工作 域 西 在到刻此 in你 報名學 叾美周收 我以及費传更名為掌握下款分裡戰鐘 宿掌握有的網路特訓 如果服 技拒星自巡 我以及老句最高看拿有人特訓 如果服 技拒星自巡 我以及費最在首鬆站個商之儀在首首頁留活隮許主流量過費最在首鬆鐘고為訓 如\n",
            "\n",
            "[701 | 1718.98] loss=2.09 avg=1.53\n",
            "[702 | 1721.36] loss=1.61 avg=1.53\n",
            "[703 | 1723.73] loss=2.27 avg=1.54\n",
            "[704 | 1726.09] loss=1.70 avg=1.54\n",
            "[705 | 1728.45] loss=0.93 avg=1.54\n",
            "[706 | 1730.82] loss=0.80 avg=1.53\n",
            "[707 | 1733.19] loss=1.91 avg=1.53\n",
            "[708 | 1735.55] loss=1.80 avg=1.54\n",
            "[709 | 1737.91] loss=0.34 avg=1.52\n",
            "[710 | 1740.27] loss=1.63 avg=1.53\n",
            "[711 | 1742.62] loss=0.62 avg=1.52\n",
            "[712 | 1744.98] loss=1.68 avg=1.52\n",
            "[713 | 1747.33] loss=1.97 avg=1.52\n",
            "[714 | 1749.68] loss=1.49 avg=1.52\n",
            "[715 | 1752.04] loss=1.53 avg=1.52\n",
            "[716 | 1754.39] loss=0.97 avg=1.52\n",
            "[717 | 1756.75] loss=0.94 avg=1.51\n",
            "[718 | 1759.11] loss=2.83 avg=1.52\n",
            "[719 | 1761.47] loss=0.26 avg=1.51\n",
            "[720 | 1763.83] loss=0.31 avg=1.50\n",
            "[721 | 1766.19] loss=1.94 avg=1.50\n",
            "[722 | 1768.54] loss=1.10 avg=1.50\n",
            "[723 | 1770.90] loss=1.64 avg=1.50\n",
            "[724 | 1773.25] loss=2.18 avg=1.51\n",
            "[725 | 1775.62] loss=1.08 avg=1.50\n",
            "[726 | 1777.97] loss=1.60 avg=1.50\n",
            "[727 | 1780.33] loss=0.86 avg=1.50\n",
            "[728 | 1782.69] loss=0.86 avg=1.49\n",
            "[729 | 1785.05] loss=2.28 avg=1.50\n",
            "[730 | 1787.40] loss=0.95 avg=1.49\n",
            "[731 | 1789.75] loss=0.25 avg=1.48\n",
            "[732 | 1792.11] loss=1.61 avg=1.48\n",
            "[733 | 1794.47] loss=0.14 avg=1.47\n",
            "[734 | 1796.84] loss=0.97 avg=1.46\n",
            "[735 | 1799.20] loss=1.56 avg=1.47\n",
            "[736 | 1801.56] loss=0.34 avg=1.45\n",
            "[737 | 1803.93] loss=1.15 avg=1.45\n",
            "[738 | 1806.29] loss=0.92 avg=1.45\n",
            "[739 | 1808.64] loss=1.60 avg=1.45\n",
            "[740 | 1811.00] loss=0.93 avg=1.44\n",
            "[741 | 1813.37] loss=0.30 avg=1.43\n",
            "[742 | 1815.72] loss=1.73 avg=1.43\n",
            "[743 | 1818.08] loss=0.93 avg=1.43\n",
            "[744 | 1820.44] loss=1.02 avg=1.42\n",
            "[745 | 1822.80] loss=0.29 avg=1.41\n",
            "[746 | 1825.16] loss=0.20 avg=1.40\n",
            "[747 | 1827.52] loss=1.71 avg=1.40\n",
            "[748 | 1829.88] loss=0.97 avg=1.40\n",
            "[749 | 1832.24] loss=0.88 avg=1.39\n",
            "[750 | 1834.60] loss=1.00 avg=1.39\n",
            "[751 | 1836.96] loss=0.22 avg=1.38\n",
            "[752 | 1839.32] loss=0.90 avg=1.37\n",
            "[753 | 1841.69] loss=1.78 avg=1.38\n",
            "[754 | 1844.04] loss=0.85 avg=1.37\n",
            "[755 | 1846.40] loss=0.82 avg=1.37\n",
            "[756 | 1848.76] loss=1.70 avg=1.37\n",
            "[757 | 1851.12] loss=2.87 avg=1.39\n",
            "[758 | 1853.48] loss=2.86 avg=1.40\n",
            "[759 | 1855.84] loss=1.44 avg=1.40\n",
            "[760 | 1858.21] loss=1.74 avg=1.40\n",
            "[761 | 1860.58] loss=1.02 avg=1.40\n",
            "[762 | 1862.93] loss=2.38 avg=1.41\n",
            "[763 | 1865.30] loss=0.81 avg=1.40\n",
            "[764 | 1867.66] loss=2.36 avg=1.41\n",
            "[765 | 1870.02] loss=1.51 avg=1.41\n",
            "[766 | 1872.37] loss=2.04 avg=1.42\n",
            "[767 | 1874.74] loss=1.57 avg=1.42\n",
            "[768 | 1877.10] loss=2.95 avg=1.44\n",
            "[769 | 1879.47] loss=0.21 avg=1.43\n",
            "[770 | 1881.84] loss=0.93 avg=1.42\n",
            "[771 | 1884.19] loss=2.00 avg=1.43\n",
            "[772 | 1886.56] loss=1.57 avg=1.43\n",
            "[773 | 1888.91] loss=2.18 avg=1.44\n",
            "[774 | 1891.27] loss=0.68 avg=1.43\n",
            "[775 | 1893.63] loss=1.64 avg=1.43\n",
            "[776 | 1895.99] loss=1.40 avg=1.43\n",
            "[777 | 1898.35] loss=3.04 avg=1.45\n",
            "[778 | 1900.71] loss=3.40 avg=1.47\n",
            "[779 | 1903.06] loss=1.09 avg=1.46\n",
            "[780 | 1905.42] loss=1.08 avg=1.46\n",
            "[781 | 1907.79] loss=1.46 avg=1.46\n",
            "[782 | 1910.16] loss=1.48 avg=1.46\n",
            "[783 | 1912.51] loss=2.89 avg=1.47\n",
            "[784 | 1914.87] loss=0.86 avg=1.47\n",
            "[785 | 1917.22] loss=0.79 avg=1.46\n",
            "[786 | 1919.60] loss=0.98 avg=1.45\n",
            "[787 | 1921.96] loss=0.88 avg=1.45\n",
            "[788 | 1924.33] loss=0.97 avg=1.44\n",
            "[789 | 1926.70] loss=1.52 avg=1.44\n",
            "[790 | 1929.06] loss=0.84 avg=1.44\n",
            "[791 | 1931.43] loss=0.51 avg=1.43\n",
            "[792 | 1933.79] loss=0.24 avg=1.42\n",
            "[793 | 1936.16] loss=2.39 avg=1.43\n",
            "[794 | 1938.51] loss=1.02 avg=1.42\n",
            "[795 | 1940.87] loss=0.84 avg=1.42\n",
            "[796 | 1943.24] loss=0.93 avg=1.41\n",
            "[797 | 1945.60] loss=1.36 avg=1.41\n",
            "[798 | 1947.96] loss=2.61 avg=1.42\n",
            "[799 | 1950.31] loss=0.97 avg=1.42\n",
            "[800 | 1952.67] loss=2.23 avg=1.43\n",
            "======== SAMPLE 1 ========\n",
            "vgBERmzNUMBERbppnjjdslNUMBERtNUMBERh jmNUMBERxndpijwkpyNUMBERqmNUMBERwjNUMBERmoy NUMBERbNUMBERmfvzsNUMBERh jnzyiwjwNUMBERnzb pxNUMBERlhkdnjdbzNUMBERzNUMBER hxzklkjNUMBERdtcNUMBERdNUMBERvnpbwmNUMBERwNUMBERrNUMBERnldhbaadkNUMBER NUMBERzfpqrptnNUMBERvwvkpivNUMBERk NUMBERcNUMBERlzfmnjyNUMBERgNUMBERmNUMBERnNUMBERdlnxzsNUMBERhuyvy NUMBERk NUMBERcNUMBERzwvrrqkNUMBERtarnnq jNUMBERvzhpljrzwNUMBERt r k NUMBERtjxriNUMBERsxkvxldzskxmNUMBERztNUMBERoNUMBERqssNUMBERfjNUMBERjfNUMBERe r yuub vNUMBERnNUMBERaNUMBERfkoNUMBERlghjtNUMBERgccclljzrhNUMBERm uyfkNUMBERo vNUMBERm NUMBERkngvuoNUMBERmNUMBERjxldNUMBERrpysozNUMBERlm hgcvkztNUMBERzvNUMBERjdvbsNUMBER pzNUMBERm dznjxytjrxNUMBERpcfpafnjfNUMBERmNUMBERy NUMBERiNUMBERvfazlkxzkpfyNUMBERmNUMBERj wtzNUMBERrgyfuxNUMBERe NUMBERwNUMBERndNUMBER NUMBERfjNUMBERfNUMBERtqx nkmpjxrNUMBERzwpNUMBERsjyNUMBERsftp jzNUMBERzwzjvnysNUMBERdvbsNUMBER NUMBERrNUMBERjwrkNUMBERkxzjhdNUMBERrNUMBERzjNUMBERx lqz jtffNUMBERhqjyuvtcavjxzqjyutqjxjNUMBERyNUMBERbxdhkxzNUMBERxmtvnraiubzrp kmlfNUMBERyNUMBERyluNUMBERjNUMBERrkxfjwwpNUMBERkNUMBERuNUMBERkdvrzNUMBERmfNUMBER mdvqNUMBERzkzmwjNUMBERzjNUMBERzsft pzNUMBERqjxkgkqnzlNUMBERnNUMBERmjwNUMBERrkxeoqsrphfNUMBERnpyNUMBERcNUMBERmNUMBERpkNUMBERjzNUMBERm ky fNUMBERkxlwpjzkvNUMBER kjprkngrrhbttkdfpqk NUMBERo NUMBERq zfNUMBERfcevNUMBERrliNUMBERmNUMBERkNUMBERywNUMBERzdtxjbclkNUMBERnjzcwppNUMBERywucNUMBERg jyuvNUMBERlNUMBERn NUMBERiNUMBERkNUMBERhoojfeofNUMBERtjkNUMBERhooqsrphfNUMBERnpyNUMBERcNUMBERmNUMBERpkNUMBERjnpy uykNUMBERrpjkxaapbci wdtNUMBERgcccNUMBERzNUMBERljxz uykNUMBERrpjkvycwdscNUMBERzwqo jNUMBEReNUMBERl NUMBERl jy yNUMBERkNUMBERpjzkpixx jywgxsiglfusNUMBERn bNUMBERxNUMBERxlzjy tckNUMBERxmqNUMBERx NUMBERmqkNUMBERkNUMBERrkwyfNUMBERrvkyljNUMBERjxNUMBERhNUMBERzswfNUMBERr mcgypcm NUMBERpNUMBERqyjNUMBER NUMBERuNUMBERl jyNUMBERkvqthNUMBERtjbfljvckNUMBERxNUMBERk NUMBERkvcqzNUMBERzq tNUMBERmrui NUMBERjkNUMBERbmnfNUMBERhqkNUMBERtbxNUMBERkNUMBERs tNUMBERyzNUMBERiNUMBER ky fNUMBERkxlwpjzkvNUMBER kjprk ngr bntsNUMBERdxjp\n",
            "\n",
            "[801 | 1964.15] loss=1.31 avg=1.43\n",
            "[802 | 1966.51] loss=0.25 avg=1.41\n",
            "[803 | 1968.87] loss=1.35 avg=1.41\n",
            "[804 | 1971.23] loss=1.58 avg=1.42\n",
            "[805 | 1973.59] loss=0.81 avg=1.41\n",
            "[806 | 1975.96] loss=0.31 avg=1.40\n",
            "[807 | 1978.32] loss=0.82 avg=1.39\n",
            "[808 | 1980.68] loss=0.94 avg=1.39\n",
            "[809 | 1983.03] loss=1.66 avg=1.39\n",
            "[810 | 1985.39] loss=0.89 avg=1.39\n",
            "[811 | 1987.75] loss=0.22 avg=1.37\n",
            "[812 | 1990.10] loss=0.82 avg=1.37\n",
            "[813 | 1992.47] loss=0.92 avg=1.36\n",
            "[814 | 1994.83] loss=0.95 avg=1.36\n",
            "[815 | 1997.19] loss=1.60 avg=1.36\n",
            "[816 | 1999.55] loss=2.06 avg=1.37\n",
            "[817 | 2001.92] loss=0.37 avg=1.36\n",
            "[818 | 2004.27] loss=1.55 avg=1.36\n",
            "[819 | 2006.63] loss=1.70 avg=1.36\n",
            "[820 | 2009.00] loss=2.02 avg=1.37\n",
            "[821 | 2011.36] loss=0.15 avg=1.36\n",
            "[822 | 2013.72] loss=1.23 avg=1.36\n",
            "[823 | 2016.08] loss=0.77 avg=1.35\n",
            "[824 | 2018.44] loss=1.44 avg=1.35\n",
            "[825 | 2020.80] loss=0.78 avg=1.35\n",
            "[826 | 2023.15] loss=0.90 avg=1.34\n",
            "[827 | 2025.51] loss=0.26 avg=1.33\n",
            "[828 | 2027.87] loss=2.39 avg=1.34\n",
            "[829 | 2030.23] loss=1.32 avg=1.34\n",
            "[830 | 2032.59] loss=0.78 avg=1.34\n",
            "[831 | 2034.95] loss=0.76 avg=1.33\n",
            "[832 | 2037.32] loss=0.18 avg=1.32\n",
            "[833 | 2039.68] loss=1.51 avg=1.32\n",
            "[834 | 2042.04] loss=0.83 avg=1.32\n",
            "[835 | 2044.41] loss=0.72 avg=1.31\n",
            "[836 | 2046.79] loss=1.42 avg=1.31\n",
            "[837 | 2049.14] loss=0.17 avg=1.30\n",
            "[838 | 2051.49] loss=1.18 avg=1.30\n",
            "[839 | 2053.85] loss=0.69 avg=1.29\n",
            "[840 | 2056.22] loss=1.14 avg=1.29\n",
            "[841 | 2058.58] loss=1.43 avg=1.29\n",
            "[842 | 2060.96] loss=0.97 avg=1.29\n",
            "[843 | 2063.32] loss=1.39 avg=1.29\n",
            "[844 | 2065.69] loss=1.55 avg=1.29\n",
            "[845 | 2068.06] loss=1.99 avg=1.30\n",
            "[846 | 2070.42] loss=1.57 avg=1.30\n",
            "[847 | 2072.78] loss=0.82 avg=1.30\n",
            "[848 | 2075.14] loss=0.89 avg=1.29\n",
            "[849 | 2077.51] loss=0.23 avg=1.28\n",
            "[850 | 2079.87] loss=0.78 avg=1.28\n",
            "[851 | 2082.23] loss=0.69 avg=1.27\n",
            "[852 | 2084.60] loss=1.98 avg=1.28\n",
            "[853 | 2086.97] loss=1.43 avg=1.28\n",
            "[854 | 2089.33] loss=0.71 avg=1.28\n",
            "[855 | 2091.68] loss=0.79 avg=1.27\n",
            "[856 | 2094.04] loss=0.84 avg=1.27\n",
            "[857 | 2096.41] loss=0.10 avg=1.25\n",
            "[858 | 2098.77] loss=0.09 avg=1.24\n",
            "[859 | 2101.13] loss=0.21 avg=1.23\n",
            "[860 | 2103.50] loss=0.84 avg=1.23\n",
            "[861 | 2105.87] loss=2.08 avg=1.24\n",
            "[862 | 2108.23] loss=1.32 avg=1.24\n",
            "[863 | 2110.60] loss=1.57 avg=1.24\n",
            "[864 | 2112.96] loss=1.48 avg=1.24\n",
            "[865 | 2115.33] loss=0.20 avg=1.23\n",
            "[866 | 2117.69] loss=2.22 avg=1.24\n",
            "[867 | 2120.06] loss=0.79 avg=1.24\n",
            "[868 | 2122.42] loss=0.90 avg=1.24\n",
            "[869 | 2124.79] loss=0.77 avg=1.23\n",
            "[870 | 2127.16] loss=0.81 avg=1.23\n",
            "[871 | 2129.52] loss=0.16 avg=1.22\n",
            "[872 | 2131.88] loss=1.27 avg=1.22\n",
            "[873 | 2134.24] loss=0.24 avg=1.21\n",
            "[874 | 2136.61] loss=0.78 avg=1.20\n",
            "[875 | 2138.98] loss=1.99 avg=1.21\n",
            "[876 | 2141.34] loss=0.69 avg=1.20\n",
            "[877 | 2143.69] loss=1.44 avg=1.21\n",
            "[878 | 2146.07] loss=0.20 avg=1.20\n",
            "[879 | 2148.44] loss=0.91 avg=1.19\n",
            "[880 | 2150.81] loss=0.35 avg=1.19\n",
            "[881 | 2153.16] loss=2.61 avg=1.20\n",
            "[882 | 2155.52] loss=0.71 avg=1.20\n",
            "[883 | 2157.88] loss=1.25 avg=1.20\n",
            "[884 | 2160.24] loss=0.76 avg=1.19\n",
            "[885 | 2162.59] loss=1.47 avg=1.19\n",
            "[886 | 2164.96] loss=1.61 avg=1.20\n",
            "[887 | 2167.32] loss=1.29 avg=1.20\n",
            "[888 | 2169.68] loss=1.63 avg=1.20\n",
            "[889 | 2172.04] loss=2.33 avg=1.21\n",
            "[890 | 2174.40] loss=2.69 avg=1.23\n",
            "[891 | 2176.76] loss=1.47 avg=1.23\n",
            "[892 | 2179.12] loss=0.66 avg=1.23\n",
            "[893 | 2181.48] loss=0.76 avg=1.22\n",
            "[894 | 2183.84] loss=0.56 avg=1.21\n",
            "[895 | 2186.21] loss=1.57 avg=1.22\n",
            "[896 | 2188.57] loss=1.32 avg=1.22\n",
            "[897 | 2190.94] loss=1.31 avg=1.22\n",
            "[898 | 2193.32] loss=1.84 avg=1.23\n",
            "[899 | 2195.69] loss=0.13 avg=1.22\n",
            "[900 | 2198.06] loss=1.44 avg=1.22\n",
            "======== SAMPLE 1 ========\n",
            "rhhcnekojjqrphbwgadcycdp NUMBERqgubytdpNUMBERpqhfbwqbfydlNUMBERvNUMBERhfii hNUMBERkhc bNUMBERkfztcqgubystpfdpNUMBERvNUMBERkfiiii jNUMBERkNUMBERzgrsstpNUMBERmk jvxnhwm rpemujdNUMBERlx ukNUMBERzgrsljtsstpNUMBERkNUMBERxkNUMBERzhqvmzbxgNUMBERjNUMBERndNUMBERkzNUMBERljdNUMBERmrlpxromuNUMBERhwmriNUMBERoNUMBERfbywgNUMBERjNUMBERjNUMBERywplnqjNUMBERoNUMBERekywnn NUMBERjNUMBERwqNUMBERoNUMBERekywnnp bNUMBERhqkpNUMBERywplnpd bncnhpsrhqNUMBERjbzttbpNUMBERkjNUMBERnNUMBERbvkNUMBERnbpNUMBERohlrrNUMBERfhddd xy dNUMBERkqrjc om pccmulhyfq cploodhngpymNUMBEResw v wwmaNUMBERq kywogrpvtNUMBERnkNUMBER qNUMBERoep aNUMBERjnwwpxeexq yhivti koqptksjjv NUMBERmoxNUMBERsfsysvy jxk myfmjNUMBERywpln axnNUMBERq pncNUMBERmqNUMBERpNUMBERpNUMBERpNUMBERpNUMBERpNUMBERpNUMBERpNUMBER pNUMBERpnNUMBERoepaNUMBERnq pakkNUMBERkNUMBERq n drzkpxvx NUMBERslfvwlNUMBERt sdh pajcNUMBERzdmNUMBERywptkwn bzwtoNUMBERoNUMBERmkNUMBERpNUMBERwgqhvldlwyuyqwNUMBERylqNUMBERt dniNUMBERq pakkNUMBERkNUMBERq n drzk ulNUMBERq pakkNUMBERk NUMBERq bNUMBERzwtoNUMBERoNUMBERmkNUMBERpNUMBERwgqhvldlwyuyqwNUMBERoplqNUMBERljw iNUMBERxjn bjNUMBERyyyddkpxoljzxcNUMBERn btNUMBERncf rhv anwerp dNUMBERwtpNUMBERe nomNUMBERq bspnkjNUMBERyvqcfhhlzkvhnrgpucNUMBERyNUMBER wbhp NUMBERjNUMBERrtvhmcNUMBERq jNUMBERoNUMBERmkNUMBERpNUMBERwgqhvldlwyuyqwNUMBERoplqNUMBERljw iNUMBERxwkpxroNUMBERiNUMBERylNUMBERq NUMBERpls jNUMBERoNUMBER mkNUMBERpNUMBERwgqhvldlwyuyqwNUMBERoplqNUMBERlkdlebdNUMBERjfhjvxkvhyodg swsNUMBERwtwztNUMBERapu NUMBERfqhvwnppvmtbtNUMBERs hpNUMBERvxyjNUMBERkofehaoNUMBERvucsNUMBERzqcgNUMBERjNUMBERhNUMBER wsywNUMBERmrbwtic NUMBERpqwvtNUMBERkzzwkNUMBERiargeNUMBERqjcfgwNUMBERmrbtiwd pakfjcNUMBERvxyyqhzqNUMBERyplhicNUMBERp jNUMBERoNUMBERmdpNUMBERpj qwvchfNUMBERmNUMBERueNUMBERt jNUMBERoNUMBERmkNUMBERpNUMBERwgqhvldlwyuyqwNUMBERoplqNUMBERlkdrpNUMBERwysubNUMBERpcgvNUMBERq hph hgNUMBERtwnjhksstNUMBERpjsNUMBERbNUMBERtnslfjbk itsNUMBERm aNUMBERd a NUMBERmlbiNUMBERngNUMBERgmpllNUMBERxNUMBERmlllqNUMBERq NUMBERrhgnxviNUMBERoNUMBERvucfmjNUMBERrkjqplpj wsipNUMBERsigmlNUMBERtbkubqcNUMBERwfchjxoqcsnnk\n",
            "\n",
            "[901 | 2209.62] loss=0.77 avg=1.21\n",
            "[902 | 2211.99] loss=0.71 avg=1.21\n",
            "[903 | 2214.38] loss=0.57 avg=1.20\n",
            "[904 | 2216.74] loss=0.72 avg=1.20\n",
            "[905 | 2219.11] loss=1.13 avg=1.20\n",
            "[906 | 2221.47] loss=0.70 avg=1.19\n",
            "[907 | 2223.83] loss=1.70 avg=1.20\n",
            "[908 | 2226.20] loss=0.69 avg=1.19\n",
            "[909 | 2228.57] loss=0.22 avg=1.18\n",
            "[910 | 2230.93] loss=1.69 avg=1.19\n",
            "[911 | 2233.30] loss=0.09 avg=1.18\n",
            "[912 | 2235.67] loss=0.64 avg=1.17\n",
            "[913 | 2238.03] loss=1.77 avg=1.18\n",
            "[914 | 2240.40] loss=0.72 avg=1.17\n",
            "[915 | 2242.75] loss=0.58 avg=1.17\n",
            "[916 | 2245.12] loss=0.10 avg=1.16\n",
            "[917 | 2247.48] loss=0.09 avg=1.14\n",
            "[918 | 2249.84] loss=1.21 avg=1.15\n",
            "[919 | 2252.21] loss=1.47 avg=1.15\n",
            "[920 | 2254.56] loss=0.79 avg=1.15\n",
            "[921 | 2256.92] loss=0.19 avg=1.14\n",
            "[922 | 2259.27] loss=1.22 avg=1.14\n",
            "[923 | 2261.62] loss=1.82 avg=1.14\n",
            "[924 | 2263.98] loss=1.39 avg=1.15\n",
            "[925 | 2266.34] loss=1.53 avg=1.15\n",
            "[926 | 2268.69] loss=0.73 avg=1.15\n",
            "[927 | 2271.03] loss=2.22 avg=1.16\n",
            "[928 | 2273.38] loss=1.25 avg=1.16\n",
            "[929 | 2275.74] loss=0.64 avg=1.15\n",
            "[930 | 2278.09] loss=1.88 avg=1.16\n",
            "[931 | 2280.45] loss=1.09 avg=1.16\n",
            "[932 | 2282.81] loss=0.60 avg=1.15\n",
            "[933 | 2285.17] loss=1.61 avg=1.16\n",
            "[934 | 2287.54] loss=1.85 avg=1.16\n",
            "[935 | 2289.91] loss=1.33 avg=1.17\n",
            "[936 | 2292.28] loss=0.74 avg=1.16\n",
            "[937 | 2294.64] loss=1.42 avg=1.16\n",
            "[938 | 2297.01] loss=0.12 avg=1.15\n",
            "[939 | 2299.39] loss=0.71 avg=1.15\n",
            "[940 | 2301.77] loss=0.64 avg=1.14\n",
            "[941 | 2304.15] loss=0.12 avg=1.13\n",
            "[942 | 2306.53] loss=0.90 avg=1.13\n",
            "[943 | 2308.91] loss=0.86 avg=1.13\n",
            "[944 | 2311.28] loss=0.63 avg=1.12\n",
            "[945 | 2313.66] loss=0.70 avg=1.12\n",
            "[946 | 2316.04] loss=0.10 avg=1.11\n",
            "[947 | 2318.41] loss=0.71 avg=1.11\n",
            "[948 | 2320.78] loss=0.75 avg=1.10\n",
            "[949 | 2323.16] loss=0.26 avg=1.09\n",
            "[950 | 2325.53] loss=1.24 avg=1.10\n",
            "[951 | 2327.90] loss=1.37 avg=1.10\n",
            "[952 | 2330.27] loss=0.65 avg=1.09\n",
            "[953 | 2332.65] loss=0.80 avg=1.09\n",
            "[954 | 2335.02] loss=1.89 avg=1.10\n",
            "[955 | 2337.40] loss=0.55 avg=1.09\n",
            "[956 | 2339.77] loss=1.99 avg=1.10\n",
            "[957 | 2342.14] loss=0.54 avg=1.10\n",
            "[958 | 2344.52] loss=2.00 avg=1.11\n",
            "[959 | 2346.88] loss=1.10 avg=1.11\n",
            "[960 | 2349.24] loss=1.15 avg=1.11\n",
            "[961 | 2351.61] loss=0.13 avg=1.10\n",
            "[962 | 2353.97] loss=0.85 avg=1.09\n",
            "[963 | 2356.35] loss=0.29 avg=1.09\n",
            "[964 | 2358.71] loss=0.62 avg=1.08\n",
            "[965 | 2361.08] loss=0.79 avg=1.08\n",
            "[966 | 2363.44] loss=1.29 avg=1.08\n",
            "[967 | 2365.80] loss=2.55 avg=1.09\n",
            "[968 | 2368.16] loss=0.75 avg=1.09\n",
            "[969 | 2370.53] loss=0.13 avg=1.08\n",
            "[970 | 2372.88] loss=0.70 avg=1.08\n",
            "[971 | 2375.24] loss=0.73 avg=1.07\n",
            "[972 | 2377.60] loss=1.19 avg=1.08\n",
            "[973 | 2379.96] loss=2.58 avg=1.09\n",
            "[974 | 2382.32] loss=0.13 avg=1.08\n",
            "[975 | 2384.68] loss=0.73 avg=1.08\n",
            "[976 | 2387.05] loss=0.38 avg=1.07\n",
            "[977 | 2389.41] loss=0.68 avg=1.07\n",
            "[978 | 2391.76] loss=0.15 avg=1.06\n",
            "[979 | 2394.12] loss=1.34 avg=1.06\n",
            "[980 | 2396.47] loss=2.15 avg=1.07\n",
            "[981 | 2398.83] loss=0.57 avg=1.07\n",
            "[982 | 2401.18] loss=0.47 avg=1.06\n",
            "[983 | 2403.55] loss=0.73 avg=1.06\n",
            "[984 | 2405.91] loss=1.08 avg=1.06\n",
            "[985 | 2408.27] loss=0.11 avg=1.05\n",
            "[986 | 2410.63] loss=0.73 avg=1.04\n",
            "[987 | 2412.99] loss=0.51 avg=1.04\n",
            "[988 | 2415.35] loss=0.13 avg=1.03\n",
            "[989 | 2417.71] loss=0.88 avg=1.03\n",
            "[990 | 2420.06] loss=0.55 avg=1.02\n",
            "[991 | 2422.42] loss=0.85 avg=1.02\n",
            "[992 | 2424.77] loss=1.71 avg=1.03\n",
            "[993 | 2427.13] loss=0.60 avg=1.02\n",
            "[994 | 2429.49] loss=1.96 avg=1.03\n",
            "[995 | 2431.85] loss=2.05 avg=1.04\n",
            "[996 | 2434.21] loss=1.05 avg=1.04\n",
            "[997 | 2436.57] loss=0.60 avg=1.04\n",
            "[998 | 2438.93] loss=1.45 avg=1.04\n",
            "[999 | 2441.28] loss=0.76 avg=1.04\n",
            "[1000 | 2443.64] loss=0.68 avg=1.04\n",
            "Saving checkpoint/run1/model-1000\n",
            "NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx-lrqCkEvTO"
      },
      "source": [
        "with open('generated_spam.txt', 'w') as f:\n",
        "  for i in range(250):\n",
        "    text = gpt2.generate(sess_phish, return_as_list=True)[0]\n",
        "    f.write('%s\\n\\n\\n\\n\\n\\n\\n\\n\\n' % text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJVPORPonkTs"
      },
      "source": [
        ""
      ]
    }
  ]
}