{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HamEmails.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMof64cwdyd0QEGlQmth6qa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegeeklife/Phishing-Email-With-GPT-2/blob/main/HamEmails.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITo9ZEXoPY5Y"
      },
      "source": [
        " ### Script for Generating Non-phishing emails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcLt8MamNNo5",
        "outputId": "4dc8a1c0-c1f4-462f-fe43-c868b785d0e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky4WXKmRgXUO",
        "outputId": "21566c06-f609-47ac-c263-13dde2619d00"
      },
      "source": [
        "# ONLY RUN ONCE\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!mkdir HamEmail\n",
        "%cd HamEmail/\n",
        "!git clone https://github.com/openai/gpt-2.git\n",
        "%cd cd gpt-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "mkdir: cannot create directory ‘HamEmail’: File exists\n",
            "/content/drive/My Drive/HamEmail\n",
            "fatal: destination path 'gpt-2' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'cd gpt-2'\n",
            "/content/drive/My Drive/HamEmail\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCUpqDtYTRHC",
        "outputId": "ae8ebf68-f955-410a-e185-ba9b4494d5db"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/HamEmail/gpt-2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/HamEmail/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cggOf1-CTgB2",
        "outputId": "006462fb-2c42-4b0f-8144-d205a6c36ce3"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbf4KhiVToOv",
        "outputId": "ab1d53e2-811e-4866-99b9-473a2bf41bbe"
      },
      "source": [
        "!pip3 install gpt-2-simple"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.7/dist-packages (0.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.19.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqnrXYdoTvnp",
        "outputId": "9f8edbad-65d9-44b9-db9c-d9b6f31812d0"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk \n",
        "import spacy\n",
        "import gpt_2_simple as gpt2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKrj6aTwTyye"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBRGJXiIS8TG"
      },
      "source": [
        "phish_file = \"spam_or_not_spam.csv\"\n",
        "df = pd.read_csv(phish_file)\n",
        "df\n",
        "\n",
        "#Extract ham emails only\n",
        "ham = df[df['label']==0]\n",
        "ham = [item for item in ham['email']]\n",
        "with open('ham_mails.txt', 'w') as f:\n",
        "    for listitem in ham:\n",
        "        f.write('%s\\n\\n\\n\\n\\n\\n\\n\\n\\n' % listitem)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtjd6pPDUNIt"
      },
      "source": [
        "Fine tune GPT-2 on ham email generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oP0BxRwGUECm",
        "outputId": "8e85cccb-298f-460e-874c-7a2fce7f9fb2"
      },
      "source": [
        "session = gpt2.start_tf_sess()\n",
        "\n",
        "\n",
        "gpt2.finetune(session,\n",
        "              'ham_mails.txt',\n",
        "              model_name=model_name,\n",
        "              #reuse=True,\n",
        "              steps=1000)   # steps is max number of training steps\n",
        "\n",
        "#gpt2.load_gpt2(session)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:03<00:00,  3.60s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 661367 tokens\n",
            "Training...\n",
            "[1 | 8.85] loss=3.99 avg=3.99\n",
            "[2 | 11.15] loss=3.80 avg=3.90\n",
            "[3 | 13.46] loss=4.37 avg=4.06\n",
            "[4 | 15.79] loss=3.53 avg=3.92\n",
            "[5 | 18.13] loss=3.96 avg=3.93\n",
            "[6 | 20.48] loss=4.11 avg=3.96\n",
            "[7 | 22.87] loss=3.01 avg=3.82\n",
            "[8 | 25.28] loss=4.06 avg=3.85\n",
            "[9 | 27.71] loss=3.64 avg=3.83\n",
            "[10 | 30.17] loss=3.63 avg=3.81\n",
            "[11 | 32.61] loss=4.26 avg=3.85\n",
            "[12 | 35.09] loss=4.03 avg=3.86\n",
            "[13 | 37.58] loss=3.54 avg=3.84\n",
            "[14 | 40.08] loss=3.89 avg=3.84\n",
            "[15 | 42.56] loss=3.83 avg=3.84\n",
            "[16 | 45.04] loss=3.43 avg=3.81\n",
            "[17 | 47.49] loss=3.49 avg=3.79\n",
            "[18 | 49.91] loss=3.84 avg=3.80\n",
            "[19 | 52.32] loss=3.62 avg=3.78\n",
            "[20 | 54.72] loss=3.87 avg=3.79\n",
            "[21 | 57.10] loss=3.26 avg=3.76\n",
            "[22 | 59.46] loss=3.09 avg=3.73\n",
            "[23 | 61.82] loss=3.33 avg=3.71\n",
            "[24 | 64.17] loss=3.90 avg=3.72\n",
            "[25 | 66.51] loss=3.47 avg=3.71\n",
            "[26 | 68.85] loss=3.17 avg=3.68\n",
            "[27 | 71.18] loss=3.94 avg=3.69\n",
            "[28 | 73.50] loss=3.65 avg=3.69\n",
            "[29 | 75.83] loss=3.51 avg=3.69\n",
            "[30 | 78.14] loss=3.71 avg=3.69\n",
            "[31 | 80.46] loss=3.40 avg=3.68\n",
            "[32 | 82.77] loss=3.76 avg=3.68\n",
            "[33 | 85.10] loss=3.72 avg=3.68\n",
            "[34 | 87.42] loss=3.61 avg=3.68\n",
            "[35 | 89.76] loss=3.12 avg=3.66\n",
            "[36 | 92.09] loss=2.84 avg=3.63\n",
            "[37 | 94.42] loss=3.92 avg=3.64\n",
            "[38 | 96.76] loss=3.56 avg=3.64\n",
            "[39 | 99.10] loss=3.10 avg=3.62\n",
            "[40 | 101.47] loss=3.61 avg=3.62\n",
            "[41 | 103.82] loss=3.53 avg=3.62\n",
            "[42 | 106.19] loss=2.83 avg=3.60\n",
            "[43 | 108.56] loss=3.54 avg=3.59\n",
            "[44 | 110.94] loss=3.17 avg=3.58\n",
            "[45 | 113.32] loss=3.22 avg=3.57\n",
            "[46 | 115.71] loss=3.45 avg=3.57\n",
            "[47 | 118.09] loss=3.15 avg=3.56\n",
            "[48 | 120.48] loss=3.59 avg=3.56\n",
            "[49 | 122.86] loss=2.90 avg=3.54\n",
            "[50 | 125.25] loss=3.11 avg=3.53\n",
            "[51 | 127.63] loss=3.29 avg=3.53\n",
            "[52 | 130.03] loss=3.50 avg=3.52\n",
            "[53 | 132.41] loss=3.78 avg=3.53\n",
            "[54 | 134.79] loss=3.38 avg=3.53\n",
            "[55 | 137.16] loss=3.45 avg=3.53\n",
            "[56 | 139.54] loss=3.61 avg=3.53\n",
            "[57 | 141.91] loss=3.38 avg=3.52\n",
            "[58 | 144.29] loss=3.54 avg=3.52\n",
            "[59 | 146.66] loss=3.56 avg=3.53\n",
            "[60 | 149.02] loss=3.68 avg=3.53\n",
            "[61 | 151.37] loss=3.20 avg=3.52\n",
            "[62 | 153.73] loss=3.50 avg=3.52\n",
            "[63 | 156.09] loss=3.82 avg=3.53\n",
            "[64 | 158.45] loss=3.75 avg=3.53\n",
            "[65 | 160.81] loss=3.92 avg=3.54\n",
            "[66 | 163.17] loss=3.29 avg=3.53\n",
            "[67 | 165.53] loss=3.58 avg=3.54\n",
            "[68 | 167.88] loss=3.22 avg=3.53\n",
            "[69 | 170.23] loss=3.55 avg=3.53\n",
            "[70 | 172.59] loss=3.54 avg=3.53\n",
            "[71 | 174.94] loss=3.25 avg=3.52\n",
            "[72 | 177.30] loss=3.39 avg=3.52\n",
            "[73 | 179.66] loss=3.41 avg=3.52\n",
            "[74 | 182.01] loss=3.12 avg=3.51\n",
            "[75 | 184.36] loss=3.23 avg=3.51\n",
            "[76 | 186.72] loss=3.59 avg=3.51\n",
            "[77 | 189.08] loss=3.04 avg=3.50\n",
            "[78 | 191.44] loss=3.61 avg=3.50\n",
            "[79 | 193.81] loss=3.17 avg=3.50\n",
            "[80 | 196.17] loss=3.12 avg=3.49\n",
            "[81 | 198.52] loss=3.35 avg=3.49\n",
            "[82 | 200.89] loss=3.31 avg=3.48\n",
            "[83 | 203.26] loss=2.70 avg=3.47\n",
            "[84 | 205.63] loss=3.48 avg=3.47\n",
            "[85 | 207.98] loss=3.69 avg=3.47\n",
            "[86 | 210.34] loss=3.38 avg=3.47\n",
            "[87 | 212.71] loss=3.03 avg=3.46\n",
            "[88 | 215.09] loss=3.54 avg=3.47\n",
            "[89 | 217.44] loss=3.95 avg=3.47\n",
            "[90 | 219.81] loss=3.23 avg=3.47\n",
            "[91 | 222.18] loss=3.14 avg=3.46\n",
            "[92 | 224.54] loss=3.28 avg=3.46\n",
            "[93 | 226.91] loss=2.74 avg=3.45\n",
            "[94 | 229.29] loss=3.37 avg=3.45\n",
            "[95 | 231.66] loss=3.16 avg=3.44\n",
            "[96 | 234.03] loss=3.54 avg=3.44\n",
            "[97 | 236.39] loss=2.96 avg=3.44\n",
            "[98 | 238.77] loss=2.81 avg=3.43\n",
            "[99 | 241.15] loss=3.19 avg=3.42\n",
            "[100 | 243.52] loss=3.11 avg=3.42\n",
            "======== SAMPLE 1 ========\n",
            " t t a g t i g a t i g a t i g a t i g a t i g a t i g a t i g b t i g a t i g g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g a t i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i g i t t am my heart the joys that lie in the power of the unyielding will is this the heart of death that s there is that very heart that you can never beat so feel all the love you cherish and that cannot love a weak heart is in no way a weakness but a joy and a joy for all the world that is that joy is always here and only here and always for all this is the heart of death here it is the heart of life here and only here and only for all this is the heart of love here and only for all this are the hearts that are there because it is always there and only here and always for all this is the heart of death here you can never beat it and only now it is this strength that makes you the heart of death and only now is it here and only now let it be there and only now and only for all this are\n",
            "\n",
            "[101 | 256.21] loss=3.16 avg=3.41\n",
            "[102 | 258.59] loss=3.58 avg=3.42\n",
            "[103 | 260.96] loss=3.39 avg=3.42\n",
            "[104 | 263.34] loss=2.68 avg=3.40\n",
            "[105 | 265.71] loss=2.91 avg=3.40\n",
            "[106 | 268.09] loss=3.26 avg=3.40\n",
            "[107 | 270.46] loss=2.99 avg=3.39\n",
            "[108 | 272.84] loss=3.26 avg=3.39\n",
            "[109 | 275.21] loss=3.02 avg=3.38\n",
            "[110 | 277.60] loss=3.28 avg=3.38\n",
            "[111 | 279.98] loss=3.35 avg=3.38\n",
            "[112 | 282.36] loss=3.27 avg=3.38\n",
            "[113 | 284.74] loss=3.50 avg=3.38\n",
            "[114 | 287.11] loss=2.73 avg=3.37\n",
            "[115 | 289.49] loss=2.81 avg=3.36\n",
            "[116 | 291.87] loss=3.13 avg=3.36\n",
            "[117 | 294.25] loss=2.99 avg=3.35\n",
            "[118 | 296.64] loss=3.35 avg=3.35\n",
            "[119 | 299.02] loss=2.98 avg=3.35\n",
            "[120 | 301.39] loss=3.07 avg=3.34\n",
            "[121 | 303.77] loss=3.44 avg=3.35\n",
            "[122 | 306.16] loss=2.89 avg=3.34\n",
            "[123 | 308.54] loss=3.54 avg=3.34\n",
            "[124 | 310.93] loss=3.09 avg=3.34\n",
            "[125 | 313.31] loss=3.40 avg=3.34\n",
            "[126 | 315.69] loss=3.52 avg=3.34\n",
            "[127 | 318.06] loss=3.36 avg=3.34\n",
            "[128 | 320.44] loss=3.27 avg=3.34\n",
            "[129 | 322.83] loss=3.19 avg=3.34\n",
            "[130 | 325.20] loss=3.00 avg=3.33\n",
            "[131 | 327.58] loss=2.88 avg=3.33\n",
            "[132 | 329.96] loss=3.39 avg=3.33\n",
            "[133 | 332.33] loss=3.20 avg=3.33\n",
            "[134 | 334.71] loss=2.81 avg=3.32\n",
            "[135 | 337.08] loss=2.61 avg=3.31\n",
            "[136 | 339.45] loss=2.62 avg=3.30\n",
            "[137 | 341.82] loss=2.82 avg=3.29\n",
            "[138 | 344.20] loss=3.12 avg=3.29\n",
            "[139 | 346.57] loss=2.93 avg=3.29\n",
            "[140 | 348.94] loss=3.07 avg=3.28\n",
            "[141 | 351.31] loss=3.15 avg=3.28\n",
            "[142 | 353.69] loss=3.22 avg=3.28\n",
            "[143 | 356.06] loss=3.16 avg=3.28\n",
            "[144 | 358.43] loss=3.57 avg=3.28\n",
            "[145 | 360.80] loss=2.71 avg=3.28\n",
            "[146 | 363.17] loss=2.83 avg=3.27\n",
            "[147 | 365.54] loss=3.26 avg=3.27\n",
            "[148 | 367.91] loss=2.98 avg=3.27\n",
            "[149 | 370.27] loss=3.10 avg=3.27\n",
            "[150 | 372.64] loss=2.98 avg=3.26\n",
            "[151 | 375.01] loss=2.73 avg=3.25\n",
            "[152 | 377.38] loss=3.48 avg=3.26\n",
            "[153 | 379.75] loss=2.86 avg=3.25\n",
            "[154 | 382.11] loss=3.12 avg=3.25\n",
            "[155 | 384.49] loss=3.15 avg=3.25\n",
            "[156 | 386.86] loss=3.26 avg=3.25\n",
            "[157 | 389.23] loss=3.08 avg=3.25\n",
            "[158 | 391.60] loss=3.47 avg=3.25\n",
            "[159 | 393.97] loss=2.90 avg=3.25\n",
            "[160 | 396.35] loss=3.00 avg=3.24\n",
            "[161 | 398.72] loss=2.67 avg=3.24\n",
            "[162 | 401.08] loss=2.41 avg=3.23\n",
            "[163 | 403.44] loss=2.82 avg=3.22\n",
            "[164 | 405.81] loss=2.99 avg=3.22\n",
            "[165 | 408.19] loss=2.98 avg=3.21\n",
            "[166 | 410.56] loss=3.06 avg=3.21\n",
            "[167 | 412.92] loss=2.60 avg=3.20\n",
            "[168 | 415.29] loss=3.03 avg=3.20\n",
            "[169 | 417.65] loss=2.82 avg=3.20\n",
            "[170 | 420.02] loss=2.84 avg=3.19\n",
            "[171 | 422.39] loss=2.94 avg=3.19\n",
            "[172 | 424.74] loss=3.07 avg=3.19\n",
            "[173 | 427.11] loss=3.02 avg=3.19\n",
            "[174 | 429.46] loss=3.19 avg=3.19\n",
            "[175 | 431.83] loss=3.36 avg=3.19\n",
            "[176 | 434.20] loss=3.16 avg=3.19\n",
            "[177 | 436.57] loss=2.68 avg=3.18\n",
            "[178 | 438.93] loss=2.90 avg=3.18\n",
            "[179 | 441.30] loss=3.10 avg=3.18\n",
            "[180 | 443.67] loss=3.28 avg=3.18\n",
            "[181 | 446.03] loss=3.16 avg=3.18\n",
            "[182 | 448.40] loss=2.81 avg=3.17\n",
            "[183 | 450.77] loss=2.60 avg=3.17\n",
            "[184 | 453.13] loss=3.13 avg=3.17\n",
            "[185 | 455.50] loss=2.92 avg=3.16\n",
            "[186 | 457.86] loss=2.67 avg=3.16\n",
            "[187 | 460.22] loss=3.16 avg=3.16\n",
            "[188 | 462.58] loss=2.84 avg=3.16\n",
            "[189 | 464.95] loss=2.96 avg=3.15\n",
            "[190 | 467.31] loss=2.90 avg=3.15\n",
            "[191 | 469.68] loss=3.10 avg=3.15\n",
            "[192 | 472.05] loss=2.63 avg=3.14\n",
            "[193 | 474.40] loss=3.42 avg=3.15\n",
            "[194 | 476.77] loss=3.12 avg=3.15\n",
            "[195 | 479.14] loss=2.74 avg=3.14\n",
            "[196 | 481.51] loss=3.19 avg=3.14\n",
            "[197 | 483.88] loss=3.04 avg=3.14\n",
            "[198 | 486.25] loss=2.67 avg=3.14\n",
            "[199 | 488.61] loss=2.76 avg=3.13\n",
            "[200 | 490.97] loss=2.95 avg=3.13\n",
            "======== SAMPLE 1 ========\n",
            " until the original release it became a must for most users because it enabled developers to create apps for any platform and allow users to build their own web apps without having to worry about copyright disputes and be legally bound by any laws they might impose on apps and let you test your apps with a trusted server or service URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " date sun dated NUMBER aug NUMBER NUMBER NUMBER NUMBER NUMBER scosk net wrote URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type html lang english date sat NUMBER NUMBER NUMBER NUMBER NUMBER from scosk org sent monday september NUMBER NUMBER NUMBER NUMBER date adam l beberg wrote URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type html lang english date sat NUMBER NUMBER NUMBER NUMBER NUMBER ersatz net wrote adam l beberg writes URL a lot of people think they know everything about programming and web development in a way that they don t \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sat NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday october NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type html lang english date sunday october NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thoughtgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday october NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday october NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type html lang english date sun NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sun NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sun NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sun NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sun NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by thinkgeek welcome to geek heaven URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url type xml date sunday NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER this URL email is sponsored by\n",
            "\n",
            "[201 | 502.57] loss=3.24 avg=3.13\n",
            "[202 | 504.94] loss=2.41 avg=3.12\n",
            "[203 | 507.30] loss=3.30 avg=3.12\n",
            "[204 | 509.67] loss=3.07 avg=3.12\n",
            "[205 | 512.04] loss=2.51 avg=3.12\n",
            "[206 | 514.40] loss=2.59 avg=3.11\n",
            "[207 | 516.76] loss=3.13 avg=3.11\n",
            "[208 | 519.13] loss=2.58 avg=3.10\n",
            "[209 | 521.48] loss=2.62 avg=3.10\n",
            "[210 | 523.85] loss=2.49 avg=3.09\n",
            "[211 | 526.21] loss=3.00 avg=3.09\n",
            "[212 | 528.57] loss=3.05 avg=3.09\n",
            "[213 | 530.92] loss=2.61 avg=3.09\n",
            "[214 | 533.28] loss=2.82 avg=3.08\n",
            "[215 | 535.64] loss=2.79 avg=3.08\n",
            "[216 | 538.00] loss=3.18 avg=3.08\n",
            "[217 | 540.37] loss=2.94 avg=3.08\n",
            "[218 | 542.72] loss=2.81 avg=3.08\n",
            "[219 | 545.08] loss=2.62 avg=3.07\n",
            "[220 | 547.46] loss=2.76 avg=3.07\n",
            "[221 | 549.81] loss=2.63 avg=3.06\n",
            "[222 | 552.17] loss=3.05 avg=3.06\n",
            "[223 | 554.53] loss=3.09 avg=3.06\n",
            "[224 | 556.88] loss=2.79 avg=3.06\n",
            "[225 | 559.24] loss=2.57 avg=3.05\n",
            "[226 | 561.60] loss=2.64 avg=3.05\n",
            "[227 | 563.96] loss=2.78 avg=3.05\n",
            "[228 | 566.32] loss=2.98 avg=3.05\n",
            "[229 | 568.67] loss=2.95 avg=3.04\n",
            "[230 | 571.04] loss=3.40 avg=3.05\n",
            "[231 | 573.41] loss=3.22 avg=3.05\n",
            "[232 | 575.78] loss=2.83 avg=3.05\n",
            "[233 | 578.14] loss=2.72 avg=3.04\n",
            "[234 | 580.50] loss=2.69 avg=3.04\n",
            "[235 | 582.87] loss=2.48 avg=3.03\n",
            "[236 | 585.23] loss=2.81 avg=3.03\n",
            "[237 | 587.60] loss=2.17 avg=3.02\n",
            "[238 | 589.96] loss=2.37 avg=3.01\n",
            "[239 | 592.32] loss=2.88 avg=3.01\n",
            "[240 | 594.68] loss=2.98 avg=3.01\n",
            "[241 | 597.05] loss=2.70 avg=3.01\n",
            "[242 | 599.40] loss=2.76 avg=3.01\n",
            "[243 | 601.78] loss=3.29 avg=3.01\n",
            "[244 | 604.16] loss=2.43 avg=3.00\n",
            "[245 | 606.51] loss=3.03 avg=3.00\n",
            "[246 | 608.87] loss=2.79 avg=3.00\n",
            "[247 | 611.24] loss=3.21 avg=3.00\n",
            "[248 | 613.60] loss=2.94 avg=3.00\n",
            "[249 | 615.97] loss=2.88 avg=3.00\n",
            "[250 | 618.33] loss=2.43 avg=3.00\n",
            "[251 | 620.68] loss=2.45 avg=2.99\n",
            "[252 | 623.05] loss=2.82 avg=2.99\n",
            "[253 | 625.40] loss=2.08 avg=2.98\n",
            "[254 | 627.76] loss=2.33 avg=2.97\n",
            "[255 | 630.12] loss=2.36 avg=2.96\n",
            "[256 | 632.48] loss=3.12 avg=2.97\n",
            "[257 | 634.84] loss=2.81 avg=2.96\n",
            "[258 | 637.20] loss=2.90 avg=2.96\n",
            "[259 | 639.57] loss=2.72 avg=2.96\n",
            "[260 | 641.94] loss=2.68 avg=2.96\n",
            "[261 | 644.31] loss=2.75 avg=2.96\n",
            "[262 | 646.67] loss=2.64 avg=2.95\n",
            "[263 | 649.03] loss=2.69 avg=2.95\n",
            "[264 | 651.41] loss=2.77 avg=2.95\n",
            "[265 | 653.76] loss=2.49 avg=2.94\n",
            "[266 | 656.12] loss=2.59 avg=2.94\n",
            "[267 | 658.50] loss=3.21 avg=2.94\n",
            "[268 | 660.87] loss=2.88 avg=2.94\n",
            "[269 | 663.23] loss=2.90 avg=2.94\n",
            "[270 | 665.60] loss=2.54 avg=2.94\n",
            "[271 | 667.98] loss=2.74 avg=2.93\n",
            "[272 | 670.36] loss=2.70 avg=2.93\n",
            "[273 | 672.73] loss=2.95 avg=2.93\n",
            "[274 | 675.11] loss=3.29 avg=2.94\n",
            "[275 | 677.49] loss=3.01 avg=2.94\n",
            "[276 | 679.86] loss=2.77 avg=2.93\n",
            "[277 | 682.25] loss=3.05 avg=2.94\n",
            "[278 | 684.63] loss=2.50 avg=2.93\n",
            "[279 | 687.01] loss=2.64 avg=2.93\n",
            "[280 | 689.39] loss=2.76 avg=2.93\n",
            "[281 | 691.77] loss=2.69 avg=2.92\n",
            "[282 | 694.15] loss=2.77 avg=2.92\n",
            "[283 | 696.53] loss=2.56 avg=2.92\n",
            "[284 | 698.91] loss=2.43 avg=2.91\n",
            "[285 | 701.28] loss=2.03 avg=2.90\n",
            "[286 | 703.67] loss=2.70 avg=2.90\n",
            "[287 | 706.05] loss=2.50 avg=2.90\n",
            "[288 | 708.43] loss=2.68 avg=2.89\n",
            "[289 | 710.81] loss=2.85 avg=2.89\n",
            "[290 | 713.18] loss=3.00 avg=2.90\n",
            "[291 | 715.55] loss=2.56 avg=2.89\n",
            "[292 | 717.93] loss=2.66 avg=2.89\n",
            "[293 | 720.31] loss=2.69 avg=2.89\n",
            "[294 | 722.68] loss=2.60 avg=2.88\n",
            "[295 | 725.06] loss=2.88 avg=2.88\n",
            "[296 | 727.43] loss=2.64 avg=2.88\n",
            "[297 | 729.81] loss=1.95 avg=2.87\n",
            "[298 | 732.19] loss=2.65 avg=2.87\n",
            "[299 | 734.56] loss=2.99 avg=2.87\n",
            "[300 | 736.94] loss=2.89 avg=2.87\n",
            "======== SAMPLE 1 ========\n",
            "BER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUMBER NUM\n",
            "\n",
            "[301 | 748.49] loss=2.88 avg=2.87\n",
            "[302 | 750.86] loss=2.85 avg=2.87\n",
            "[303 | 753.23] loss=2.52 avg=2.87\n",
            "[304 | 755.59] loss=2.48 avg=2.86\n",
            "[305 | 757.96] loss=2.68 avg=2.86\n",
            "[306 | 760.32] loss=2.79 avg=2.86\n",
            "[307 | 762.68] loss=2.67 avg=2.86\n",
            "[308 | 765.05] loss=2.47 avg=2.85\n",
            "[309 | 767.42] loss=3.33 avg=2.86\n",
            "[310 | 769.77] loss=2.51 avg=2.86\n",
            "[311 | 772.13] loss=2.51 avg=2.85\n",
            "[312 | 774.51] loss=2.03 avg=2.84\n",
            "[313 | 776.88] loss=2.82 avg=2.84\n",
            "[314 | 779.24] loss=2.40 avg=2.84\n",
            "[315 | 781.62] loss=3.14 avg=2.84\n",
            "[316 | 783.99] loss=2.61 avg=2.84\n",
            "[317 | 786.35] loss=3.29 avg=2.84\n",
            "[318 | 788.71] loss=2.98 avg=2.85\n",
            "[319 | 791.08] loss=2.28 avg=2.84\n",
            "[320 | 793.45] loss=2.49 avg=2.84\n",
            "[321 | 795.82] loss=2.35 avg=2.83\n",
            "[322 | 798.18] loss=2.48 avg=2.83\n",
            "[323 | 800.55] loss=2.90 avg=2.83\n",
            "[324 | 802.91] loss=2.58 avg=2.83\n",
            "[325 | 805.27] loss=2.79 avg=2.83\n",
            "[326 | 807.63] loss=2.29 avg=2.82\n",
            "[327 | 810.00] loss=2.49 avg=2.82\n",
            "[328 | 812.36] loss=2.43 avg=2.81\n",
            "[329 | 814.72] loss=2.61 avg=2.81\n",
            "[330 | 817.09] loss=2.72 avg=2.81\n",
            "[331 | 819.45] loss=2.58 avg=2.81\n",
            "[332 | 821.81] loss=2.20 avg=2.80\n",
            "[333 | 824.19] loss=2.75 avg=2.80\n",
            "[334 | 826.55] loss=2.59 avg=2.80\n",
            "[335 | 828.90] loss=2.57 avg=2.80\n",
            "[336 | 831.26] loss=2.66 avg=2.79\n",
            "[337 | 833.64] loss=2.51 avg=2.79\n",
            "[338 | 836.00] loss=2.54 avg=2.79\n",
            "[339 | 838.36] loss=2.85 avg=2.79\n",
            "[340 | 840.72] loss=2.62 avg=2.79\n",
            "[341 | 843.09] loss=1.99 avg=2.78\n",
            "[342 | 845.44] loss=2.16 avg=2.77\n",
            "[343 | 847.82] loss=2.74 avg=2.77\n",
            "[344 | 850.19] loss=2.59 avg=2.77\n",
            "[345 | 852.56] loss=2.86 avg=2.77\n",
            "[346 | 854.93] loss=2.28 avg=2.77\n",
            "[347 | 857.30] loss=2.12 avg=2.76\n",
            "[348 | 859.66] loss=2.45 avg=2.76\n",
            "[349 | 862.02] loss=2.57 avg=2.75\n",
            "[350 | 864.39] loss=2.74 avg=2.75\n",
            "[351 | 866.75] loss=2.17 avg=2.75\n",
            "[352 | 869.12] loss=2.11 avg=2.74\n",
            "[353 | 871.48] loss=2.92 avg=2.74\n",
            "[354 | 873.84] loss=2.46 avg=2.74\n",
            "[355 | 876.21] loss=2.55 avg=2.74\n",
            "[356 | 878.58] loss=2.40 avg=2.74\n",
            "[357 | 880.94] loss=2.52 avg=2.73\n",
            "[358 | 883.32] loss=2.30 avg=2.73\n",
            "[359 | 885.68] loss=2.33 avg=2.72\n",
            "[360 | 888.05] loss=2.87 avg=2.73\n",
            "[361 | 890.41] loss=2.33 avg=2.72\n",
            "[362 | 892.77] loss=3.00 avg=2.73\n",
            "[363 | 895.13] loss=1.97 avg=2.72\n",
            "[364 | 897.49] loss=3.06 avg=2.72\n",
            "[365 | 899.85] loss=2.43 avg=2.72\n",
            "[366 | 902.21] loss=2.74 avg=2.72\n",
            "[367 | 904.58] loss=2.51 avg=2.72\n",
            "[368 | 906.95] loss=2.17 avg=2.71\n",
            "[369 | 909.32] loss=2.01 avg=2.70\n",
            "[370 | 911.69] loss=2.79 avg=2.70\n",
            "[371 | 914.06] loss=2.05 avg=2.70\n",
            "[372 | 916.43] loss=2.24 avg=2.69\n",
            "[373 | 918.79] loss=2.70 avg=2.69\n",
            "[374 | 921.16] loss=2.32 avg=2.69\n",
            "[375 | 923.53] loss=2.63 avg=2.69\n",
            "[376 | 925.90] loss=2.45 avg=2.69\n",
            "[377 | 928.27] loss=2.94 avg=2.69\n",
            "[378 | 930.64] loss=2.84 avg=2.69\n",
            "[379 | 933.02] loss=2.61 avg=2.69\n",
            "[380 | 935.38] loss=2.45 avg=2.69\n",
            "[381 | 937.75] loss=2.72 avg=2.69\n",
            "[382 | 940.12] loss=2.85 avg=2.69\n",
            "[383 | 942.49] loss=2.36 avg=2.69\n",
            "[384 | 944.85] loss=2.77 avg=2.69\n",
            "[385 | 947.23] loss=2.29 avg=2.68\n",
            "[386 | 949.59] loss=2.79 avg=2.68\n",
            "[387 | 951.95] loss=1.95 avg=2.68\n",
            "[388 | 954.32] loss=2.99 avg=2.68\n",
            "[389 | 956.69] loss=2.27 avg=2.67\n",
            "[390 | 959.05] loss=2.38 avg=2.67\n",
            "[391 | 961.42] loss=2.52 avg=2.67\n",
            "[392 | 963.78] loss=2.42 avg=2.67\n",
            "[393 | 966.13] loss=2.76 avg=2.67\n",
            "[394 | 968.49] loss=2.55 avg=2.67\n",
            "[395 | 970.86] loss=2.37 avg=2.66\n",
            "[396 | 973.23] loss=2.83 avg=2.67\n",
            "[397 | 975.59] loss=2.12 avg=2.66\n",
            "[398 | 977.96] loss=2.54 avg=2.66\n",
            "[399 | 980.32] loss=2.04 avg=2.65\n",
            "[400 | 982.69] loss=1.97 avg=2.65\n",
            "======== SAMPLE 1 ========\n",
            " the end but the only way the government can continue to support so called free markets is if it is allowed to continue to support terrorism in the name of free markets we must not allow these people to exist any longer \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " begin pgp signature version public key not supplied i d be willing to accept this but i would not be happy for this message to continue to be sent i m currently writing a non print secure message which you can download just click on or at URL you ve been warned this message has been sent to you from jesse rucker s email address ______________________________________________________________ email sdm net with information send NUMBER october NUMBER NUMBER NUMBER NUMBER NUMBER to pm ederson URL URL end pgp signature pao NUMBERdNUMBER NUMBERbNUMBERaNUMBER the first of what is to be perhaps the only trusted e mail programs to have its URL pages a bit more secure i d be happy to accept this message to continue to be sent i d be willing to accept this too but i would not be happy for this message to continue be sent if everyone is happy i can give you all a nice surprise by removing this message from your system pm ederson URL URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " begin pgp signature version public key not supplied URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "jesse rucker wrote URL this is a new one from me but i think the most obvious is how about adding a custom headers field to the email header field in the headers field of e mail i have no suspicion this is a very efficient way to deliver e mail to spamd as i found it quite easy however since this was posted by NUMBERtrollNUMBER i don t think anyone else has found this approach useful to their mail i m willing to put up with non reliable systems like google sending me their spamd spam headers yes or no for this question i really don t have time to write a paper on email or have you to do my research if i do write it up it will be a much simpler case the following is an email i received from yahoo the spamd corporation URL in regards to my earlier question jew huckleberry is this really possible if not better that i take it away and take the risk of writing a paper on it i don t know any of the people on this list who would do that URL NUMBER NUMBER URL not that i m in any hurry to do the writing on the wall but there has to need to be some effort make sure that the spamd corporation URL doesn t use email and that they don t advertise the existence of a product or service that can t be used to send e mail you ll only agree that they should if they choose not to take these precautions to reach this goal we should all be better off if the spamd system is truly public and free from commercial enterprises than if it is proprietary and used by individuals to do their dirty work for them the problem is not that those individuals are corrupt or immoral they were doing the talking but rather that they are corrupt or immoral to commit a crime to make money for the scam we have to remember that we are all part of this system i m willing to do research and do the research if something is deemed immoral is done to ensure cash for the person who commits it the problem is that if these individuals commit it in a way that is immoral the problem remains i don t think there s any one solution i m willing to try to move the problem to another area of concern and that is the spamd system j t gingrich URL author debug messages java NUMBER the c onlines NUMBER jdk NUMBER NUMBER NUMBER NUMBER jre jdkNUMBER s message processing framework provides an on demand process which processes the message and returns a handle to the response for the given handle such that it can be referenced by a process or a class in the client code but the on demand framework doesn t have to be invoked from the client jt gingrich URL author debug messages openssl NUMBER NUMBER NUMBER ssl openssl decmodifies encoding from us sslNUMBER to usslNUMBER which decodes to us ssl key and decmodifies the value from us ssl key the standard library of decmodifies decmodifies this with the following code the value of decmod is encoded in NUMBER for convenience and non strict safe encode as ssl is in the client this is because the library was never fully tested with the ssl scheme as decmod uses ssl as the encoding bit value is stored in the decmod part of the code it is safe to assume that decmod is safe but is not because of the scheme i have not tested this before but rather to confirm the notion that this is the best implementation i think ssl is the optimal scheme for the most important aspect of doing business with spam and using it in a wide variety of ways ssl allows email clients to send email to people it s also the ideal approach given the complexity of the task because it doesn t need to be the server\n",
            "\n",
            "[401 | 994.27] loss=2.49 avg=2.64\n",
            "[402 | 996.64] loss=2.21 avg=2.64\n",
            "[403 | 999.00] loss=1.86 avg=2.63\n",
            "[404 | 1001.38] loss=1.82 avg=2.62\n",
            "[405 | 1003.74] loss=2.17 avg=2.62\n",
            "[406 | 1006.10] loss=2.23 avg=2.62\n",
            "[407 | 1008.47] loss=1.89 avg=2.61\n",
            "[408 | 1010.84] loss=2.31 avg=2.60\n",
            "[409 | 1013.21] loss=2.08 avg=2.60\n",
            "[410 | 1015.57] loss=2.46 avg=2.60\n",
            "[411 | 1017.95] loss=1.66 avg=2.59\n",
            "[412 | 1020.30] loss=2.26 avg=2.59\n",
            "[413 | 1022.68] loss=2.36 avg=2.58\n",
            "[414 | 1025.03] loss=2.43 avg=2.58\n",
            "[415 | 1027.41] loss=2.47 avg=2.58\n",
            "[416 | 1029.77] loss=2.44 avg=2.58\n",
            "[417 | 1032.13] loss=2.14 avg=2.57\n",
            "[418 | 1034.49] loss=2.11 avg=2.57\n",
            "[419 | 1036.85] loss=2.03 avg=2.56\n",
            "[420 | 1039.23] loss=2.01 avg=2.56\n",
            "[421 | 1041.60] loss=2.15 avg=2.55\n",
            "[422 | 1043.96] loss=2.36 avg=2.55\n",
            "[423 | 1046.34] loss=1.98 avg=2.55\n",
            "[424 | 1048.70] loss=2.63 avg=2.55\n",
            "[425 | 1051.08] loss=1.96 avg=2.54\n",
            "[426 | 1053.45] loss=2.47 avg=2.54\n",
            "[427 | 1055.81] loss=2.18 avg=2.54\n",
            "[428 | 1058.19] loss=1.97 avg=2.53\n",
            "[429 | 1060.56] loss=1.78 avg=2.52\n",
            "[430 | 1062.92] loss=1.65 avg=2.51\n",
            "[431 | 1065.29] loss=2.83 avg=2.52\n",
            "[432 | 1067.66] loss=2.04 avg=2.51\n",
            "[433 | 1070.04] loss=2.29 avg=2.51\n",
            "[434 | 1072.41] loss=1.78 avg=2.50\n",
            "[435 | 1074.77] loss=2.33 avg=2.50\n",
            "[436 | 1077.13] loss=2.01 avg=2.50\n",
            "[437 | 1079.51] loss=2.24 avg=2.49\n",
            "[438 | 1081.87] loss=2.39 avg=2.49\n",
            "[439 | 1084.24] loss=2.48 avg=2.49\n",
            "[440 | 1086.61] loss=2.67 avg=2.50\n",
            "[441 | 1088.97] loss=2.93 avg=2.50\n",
            "[442 | 1091.34] loss=2.12 avg=2.50\n",
            "[443 | 1093.72] loss=1.75 avg=2.49\n",
            "[444 | 1096.08] loss=2.44 avg=2.49\n",
            "[445 | 1098.46] loss=2.10 avg=2.48\n",
            "[446 | 1100.83] loss=2.25 avg=2.48\n",
            "[447 | 1103.20] loss=1.94 avg=2.48\n",
            "[448 | 1105.58] loss=2.38 avg=2.47\n",
            "[449 | 1107.93] loss=2.41 avg=2.47\n",
            "[450 | 1110.29] loss=2.01 avg=2.47\n",
            "[451 | 1112.67] loss=2.66 avg=2.47\n",
            "[452 | 1115.04] loss=1.98 avg=2.47\n",
            "[453 | 1117.42] loss=2.36 avg=2.47\n",
            "[454 | 1119.79] loss=2.13 avg=2.46\n",
            "[455 | 1122.16] loss=2.24 avg=2.46\n",
            "[456 | 1124.53] loss=2.75 avg=2.46\n",
            "[457 | 1126.89] loss=2.93 avg=2.47\n",
            "[458 | 1129.25] loss=2.35 avg=2.47\n",
            "[459 | 1131.62] loss=2.34 avg=2.46\n",
            "[460 | 1133.99] loss=2.75 avg=2.47\n",
            "[461 | 1136.37] loss=2.35 avg=2.47\n",
            "[462 | 1138.74] loss=2.42 avg=2.47\n",
            "[463 | 1141.10] loss=2.60 avg=2.47\n",
            "[464 | 1143.47] loss=2.73 avg=2.47\n",
            "[465 | 1145.84] loss=1.94 avg=2.46\n",
            "[466 | 1148.22] loss=2.39 avg=2.46\n",
            "[467 | 1150.60] loss=1.77 avg=2.46\n",
            "[468 | 1152.97] loss=2.15 avg=2.45\n",
            "[469 | 1155.34] loss=3.03 avg=2.46\n",
            "[470 | 1157.70] loss=1.87 avg=2.45\n",
            "[471 | 1160.07] loss=2.59 avg=2.45\n",
            "[472 | 1162.43] loss=2.05 avg=2.45\n",
            "[473 | 1164.81] loss=2.60 avg=2.45\n",
            "[474 | 1167.17] loss=2.31 avg=2.45\n",
            "[475 | 1169.53] loss=2.30 avg=2.45\n",
            "[476 | 1171.91] loss=2.16 avg=2.45\n",
            "[477 | 1174.27] loss=2.22 avg=2.44\n",
            "[478 | 1176.64] loss=2.32 avg=2.44\n",
            "[479 | 1179.01] loss=2.35 avg=2.44\n",
            "[480 | 1181.38] loss=2.03 avg=2.44\n",
            "[481 | 1183.76] loss=2.64 avg=2.44\n",
            "[482 | 1186.12] loss=2.01 avg=2.44\n",
            "[483 | 1188.49] loss=2.48 avg=2.44\n",
            "[484 | 1190.87] loss=2.29 avg=2.43\n",
            "[485 | 1193.24] loss=1.96 avg=2.43\n",
            "[486 | 1195.61] loss=2.39 avg=2.43\n",
            "[487 | 1197.99] loss=2.51 avg=2.43\n",
            "[488 | 1200.36] loss=1.46 avg=2.42\n",
            "[489 | 1202.72] loss=2.22 avg=2.42\n",
            "[490 | 1205.09] loss=2.26 avg=2.42\n",
            "[491 | 1207.46] loss=2.10 avg=2.41\n",
            "[492 | 1209.83] loss=1.82 avg=2.41\n",
            "[493 | 1212.20] loss=2.29 avg=2.41\n",
            "[494 | 1214.57] loss=2.30 avg=2.41\n",
            "[495 | 1216.95] loss=2.10 avg=2.40\n",
            "[496 | 1219.32] loss=2.14 avg=2.40\n",
            "[497 | 1221.69] loss=1.80 avg=2.39\n",
            "[498 | 1224.06] loss=2.34 avg=2.39\n",
            "[499 | 1226.42] loss=2.17 avg=2.39\n",
            "[500 | 1228.79] loss=2.22 avg=2.39\n",
            "======== SAMPLE 1 ========\n",
            " had a hard time getting anything done and then as soon as the president got to church he started freaking out and i don t know why but we did everything we could to stop him from doing any damage he possibly could to the country i mean just his family it s like they don t deserve to survive maybe if our country had kids the terrorists would be out there on the loose but we don t have kids yet we don t have laws that protect them and it would be irresponsible to let that happen i mean you know how i feel about this whole parental battle between a parent and their child it s just bizarre how far we can go crazy parents just don t understand what is coming out of their kids mouth apparently even worse than they recognize they can control their kids how they wish them luck at least from my understanding the dad has a natural temper where he will just ignore it and do as he likes and the mom has no clue what he is going through she has no clue at all this isn t a generational issue i mean it could be your mom but i don t think it s a family issue he s definitely aware of it and he just wants his kids to be independent and happy he s extremely protective of his kids i mean to say to his face it could be a pretty bitter pill to swallow for some parents it can sometimes be hard for them to let their kids grow up there s certainly no shortage of people who will not defend their young people or their right to make their own decisions but for most parents it means a whole lot of hard work and determination and for the very young people in particular it doesn t happen overnight but it can happen quickly it just happens to be happening at the wrong place at the wrong time so that s it i e you can t take what you don t get the reality is that kids are formed when the nervous system gets into overdrive which is why it s kind of funny how the extreme right tends to paint them that way and the opposite is true i mean they don t want their emotions to be into excess i mean their emotions are into excess and they are trying to control their emotions to an extreme and it just seems more and more normal to them given the distractions as you point out in the article i m sure they would love an answer and i m sure they would love a solution not just yet but certainly as one friend put it he was a lot more clear headed than i was he liked to joke about it and he liked to remind himself sometimes which is always good for fun i mean i can you imagine just thinking about this and my head spinning i really don t know what to do about this but you seem to assume it doesn t affect how we feel i ll just shut up or get out of here i ve had to go for the distraction relief stuff like this you know this just felt like it would be easier if i just stayed at home and waited until i m done with the meeting the other day you know you always try to put your best foot forward you know that when you do try not everything goes your way so i ll back down from your knee jerk thinking i will eventually end up hurting you the second it s over i will see you at the venue don t go yet can t let go i was wondering about getting back into the car on c road if anyone has any more questions feel free to ask and i m sure you can answer any of them in a few days if you ve read this far you might be wondering what happened to the idea that a parent can force their child not to make his or her own choices and i don t know how that happened but there s no excuse not to make your own choices and that s exactly what happened to me that s right kids you don t want to make what your parents want to make you want to make their own choices too kids have choices they have to make and if your parents want to make them they have to make their own and they have to make that choice then you have a pretty good chance of survival if you let your impulses override your instincts then you win the argument boy i don t blame you kids but you don t blame anybody else for your struggles i blame your parents for making life choices but you don t have to blame anyone else i blame politics and the media mostly for how they can cover politics but you still have to blame the rest of us for making life choices the one time you look in the mirror and say you don t look like shit btw you look beautiful and people are jealous of that i m ugly and i m ugly and people are jealous of my ugly face you know who else to blame on something that s a function of upbringing not how it is i ll also never forgive myself for being ugly and that s the beauty of it i ll never forgive the media for how they treated me and the people i m accusing of covering me they were stupid if they hadn t been they wouldn t be running an anti news outlet it sure as hell wasn t run by an idiot if they hadn t really tried to do anything to address the crisis it wouldn t have been a newscast or a newsc\n",
            "\n",
            "[501 | 1240.39] loss=1.94 avg=2.38\n",
            "[502 | 1242.75] loss=2.15 avg=2.38\n",
            "[503 | 1245.12] loss=2.11 avg=2.38\n",
            "[504 | 1247.49] loss=1.92 avg=2.37\n",
            "[505 | 1249.86] loss=1.68 avg=2.37\n",
            "[506 | 1252.24] loss=2.33 avg=2.37\n",
            "[507 | 1254.59] loss=1.86 avg=2.36\n",
            "[508 | 1256.95] loss=2.44 avg=2.36\n",
            "[509 | 1259.31] loss=2.19 avg=2.36\n",
            "[510 | 1261.68] loss=2.21 avg=2.36\n",
            "[511 | 1264.04] loss=1.94 avg=2.36\n",
            "[512 | 1266.40] loss=1.73 avg=2.35\n",
            "[513 | 1268.76] loss=1.71 avg=2.34\n",
            "[514 | 1271.12] loss=1.38 avg=2.33\n",
            "[515 | 1273.47] loss=2.81 avg=2.34\n",
            "[516 | 1275.83] loss=1.42 avg=2.33\n",
            "[517 | 1278.19] loss=1.73 avg=2.32\n",
            "[518 | 1280.54] loss=1.92 avg=2.32\n",
            "[519 | 1282.90] loss=2.13 avg=2.32\n",
            "[520 | 1285.25] loss=2.33 avg=2.32\n",
            "[521 | 1287.61] loss=1.64 avg=2.31\n",
            "[522 | 1289.96] loss=2.23 avg=2.31\n",
            "[523 | 1292.34] loss=2.10 avg=2.31\n",
            "[524 | 1294.70] loss=1.77 avg=2.30\n",
            "[525 | 1297.07] loss=1.65 avg=2.30\n",
            "[526 | 1299.42] loss=2.58 avg=2.30\n",
            "[527 | 1301.79] loss=1.82 avg=2.29\n",
            "[528 | 1304.15] loss=1.90 avg=2.29\n",
            "[529 | 1306.53] loss=2.01 avg=2.29\n",
            "[530 | 1308.89] loss=2.07 avg=2.28\n",
            "[531 | 1311.26] loss=2.24 avg=2.28\n",
            "[532 | 1313.63] loss=1.86 avg=2.28\n",
            "[533 | 1316.01] loss=2.00 avg=2.28\n",
            "[534 | 1318.39] loss=1.92 avg=2.27\n",
            "[535 | 1320.77] loss=1.81 avg=2.27\n",
            "[536 | 1323.15] loss=1.74 avg=2.26\n",
            "[537 | 1325.52] loss=1.56 avg=2.26\n",
            "[538 | 1327.90] loss=2.05 avg=2.25\n",
            "[539 | 1330.29] loss=1.50 avg=2.25\n",
            "[540 | 1332.66] loss=1.95 avg=2.24\n",
            "[541 | 1335.04] loss=1.74 avg=2.24\n",
            "[542 | 1337.41] loss=1.84 avg=2.23\n",
            "[543 | 1339.79] loss=2.53 avg=2.24\n",
            "[544 | 1342.18] loss=2.02 avg=2.24\n",
            "[545 | 1344.56] loss=1.88 avg=2.23\n",
            "[546 | 1346.94] loss=1.40 avg=2.22\n",
            "[547 | 1349.31] loss=1.62 avg=2.22\n",
            "[548 | 1351.69] loss=1.69 avg=2.21\n",
            "[549 | 1354.07] loss=2.43 avg=2.21\n",
            "[550 | 1356.44] loss=2.08 avg=2.21\n",
            "[551 | 1358.82] loss=1.98 avg=2.21\n",
            "[552 | 1361.19] loss=1.80 avg=2.21\n",
            "[553 | 1363.58] loss=2.26 avg=2.21\n",
            "[554 | 1365.95] loss=1.72 avg=2.20\n",
            "[555 | 1368.34] loss=2.20 avg=2.20\n",
            "[556 | 1370.72] loss=1.98 avg=2.20\n",
            "[557 | 1373.09] loss=2.36 avg=2.20\n",
            "[558 | 1375.47] loss=2.43 avg=2.20\n",
            "[559 | 1377.85] loss=2.21 avg=2.20\n",
            "[560 | 1380.24] loss=2.05 avg=2.20\n",
            "[561 | 1382.62] loss=2.51 avg=2.21\n",
            "[562 | 1385.01] loss=1.80 avg=2.20\n",
            "[563 | 1387.39] loss=1.40 avg=2.19\n",
            "[564 | 1389.78] loss=2.06 avg=2.19\n",
            "[565 | 1392.16] loss=1.87 avg=2.19\n",
            "[566 | 1394.53] loss=1.73 avg=2.18\n",
            "[567 | 1396.91] loss=1.83 avg=2.18\n",
            "[568 | 1399.28] loss=1.63 avg=2.18\n",
            "[569 | 1401.66] loss=2.00 avg=2.17\n",
            "[570 | 1404.03] loss=2.40 avg=2.18\n",
            "[571 | 1406.40] loss=1.47 avg=2.17\n",
            "[572 | 1408.78] loss=1.99 avg=2.17\n",
            "[573 | 1411.15] loss=2.40 avg=2.17\n",
            "[574 | 1413.51] loss=2.12 avg=2.17\n",
            "[575 | 1415.87] loss=2.22 avg=2.17\n",
            "[576 | 1418.23] loss=1.57 avg=2.16\n",
            "[577 | 1420.59] loss=1.43 avg=2.16\n",
            "[578 | 1422.94] loss=2.50 avg=2.16\n",
            "[579 | 1425.30] loss=1.69 avg=2.15\n",
            "[580 | 1427.67] loss=2.29 avg=2.16\n",
            "[581 | 1430.03] loss=2.24 avg=2.16\n",
            "[582 | 1432.39] loss=1.64 avg=2.15\n",
            "[583 | 1434.75] loss=2.06 avg=2.15\n",
            "[584 | 1437.11] loss=1.61 avg=2.15\n",
            "[585 | 1439.48] loss=1.37 avg=2.14\n",
            "[586 | 1441.84] loss=1.68 avg=2.13\n",
            "[587 | 1444.21] loss=2.65 avg=2.14\n",
            "[588 | 1446.58] loss=2.16 avg=2.14\n",
            "[589 | 1448.96] loss=1.98 avg=2.14\n",
            "[590 | 1451.34] loss=1.83 avg=2.13\n",
            "[591 | 1453.72] loss=2.35 avg=2.14\n",
            "[592 | 1456.10] loss=2.13 avg=2.14\n",
            "[593 | 1458.48] loss=1.56 avg=2.13\n",
            "[594 | 1460.86] loss=1.73 avg=2.13\n",
            "[595 | 1463.24] loss=2.05 avg=2.13\n",
            "[596 | 1465.62] loss=1.85 avg=2.12\n",
            "[597 | 1468.01] loss=1.56 avg=2.12\n",
            "[598 | 1470.39] loss=2.37 avg=2.12\n",
            "[599 | 1472.77] loss=1.48 avg=2.11\n",
            "[600 | 1475.15] loss=2.37 avg=2.12\n",
            "======== SAMPLE 1 ========\n",
            " for our allies in the north wendys the west india pelagic penguins our partner the republic of china are suffering terribly after NUMBER penguins from mongolese africa were tragically killed while nesting in NUMBER a new report estimates that half of chicks in these penguins have microplastic fill them a procedure that is usually quite painful if necessary to remove the skin from the back of the baby to protect the chicks from drowning when the chicks are carried on ice during the day they are doused in large quantities of either warm wet or cold water and left to die it has been established that such procedures can significantly reduce the number of chicks being left to die NUMBER NUMBER of these chicks where NUMBER are endangered at a time of economic crisis it is estimated that as many as NUMBER million people could be vulnerable to infection when a baby penguin cub is born and NUMBER million at risk because of infectious diseases most chicks born to cubs are killed within hours at the shore as the mother takes her bath and seals off the water if the cubs are found to be healthy enough to go home they will be returned to their nests in the spring the cost of treating the injured chicks born to cubs is estimated to be NUMBER NUMBER million the cost of treating the sick is estimated to be NUMBER million depending upon how long the animals are in the water and on what the social groups are doing penguins make up approx half of the population of NUMBER NUMBER birds are cared for each year and care packages can typically be found for approx NUMBER NUMBER people and they are usually paid NUMBER NUMBER for each person that goes near one of the NUMBER NUMBER penguins which are kept in ny zen yorkshire ken lai personne kennen luida have established a network of more than NUMBER NUMBER computers with more computers expected in NUMBER as part of ny zen yorkshire ken lai zen yorkshire vw department manager for conservation see www wvbccentral and webee u s deputy leader wwbee ceci said while the ny zen yorkshire case supports the conservation efforts of some of the park s most endangered birds ny zenorkshire is one of seven penguins that live underground where penguins get their heat and they can breathe underwater they are also extremely vulnerable to winter deaths they re fed on almost exclusively white flour they can also be killed instantly if they are found outside water temperatures in the ny zen yorkshire case are predicted to climb to NUMBERc by next spring and to NUMBERc by NUMBER days in some areas this will mean that ny zen yorkshire owen penguins may experience a lethal temperature ranging from NUMBERc to NUMBERc they are most vulnerable to the icy north because of their short winter hide and they can easily escape from the otawanda and kippers the north wales government is already considering increasing funding for protection operations and training for ny zen yorkshire kippers see this as an opportunity to train ny zen penguins to cope in the coldest regions of the country owen penguins can become dangerously emaciated and quickly die and kippers have already lost several cubs in the event of a natural disaster the potential benefits to the country outweigh any health concerns at all in raising temperatures above NUMBERc celsius and preventing deaths the costs for these projects are enormous but the ny zen yorkshire scheme has helped to raise these costs considerably the costs of care of ny zen penguin chicks and equipment can also be included in any initial NUMBER NUMBER pledge levels the kippers will also be required to complete three miles of new road that will reduce their suffering and allow them to spend more time with their young see this as a clear opportunity to raise the costs of care and training for ny zen penguins there is a growing body of scientific and environmental evidence that ny zen penguins feed on a broad range of plant based plants and animals as well as grasshoppers and small mammals this research can help us to understand why this species lives underground owen penguins lack viable young and therefore their budget is very limited but they do have viable young which means that with the right training they could be on their way to having an adult male or a female webee conservation officers work with people who can provide useful information including educational materials and job interviews to identify and train ny zen penguins so that they can take advantage of all the benefits of sustainable breeding and care and to lead healthy and happy childhoods the cost of care and training for training and support for ny zen penguins can range from NUMBER NUMBERm EUR to NUMBER million every person can expect to pay for this care and support the cost of caring and supporting ny zen penguins is estimated to be NUMBER euro a week plus other expenses such as travel and supplies of basic necessities such as food and water could be paid for during a period when ny zen penguins\n",
            "\n",
            "[601 | 1486.69] loss=2.15 avg=2.12\n",
            "[602 | 1489.06] loss=2.32 avg=2.12\n",
            "[603 | 1491.44] loss=1.49 avg=2.11\n",
            "[604 | 1493.81] loss=2.19 avg=2.11\n",
            "[605 | 1496.19] loss=1.82 avg=2.11\n",
            "[606 | 1498.57] loss=1.52 avg=2.10\n",
            "[607 | 1500.94] loss=1.15 avg=2.09\n",
            "[608 | 1503.31] loss=2.10 avg=2.09\n",
            "[609 | 1505.68] loss=2.13 avg=2.09\n",
            "[610 | 1508.06] loss=1.16 avg=2.08\n",
            "[611 | 1510.43] loss=2.06 avg=2.08\n",
            "[612 | 1512.80] loss=2.09 avg=2.08\n",
            "[613 | 1515.16] loss=1.89 avg=2.08\n",
            "[614 | 1517.54] loss=1.48 avg=2.08\n",
            "[615 | 1519.90] loss=1.62 avg=2.07\n",
            "[616 | 1522.28] loss=2.53 avg=2.08\n",
            "[617 | 1524.66] loss=1.55 avg=2.07\n",
            "[618 | 1527.03] loss=1.77 avg=2.07\n",
            "[619 | 1529.40] loss=1.99 avg=2.07\n",
            "[620 | 1531.77] loss=2.18 avg=2.07\n",
            "[621 | 1534.13] loss=1.69 avg=2.07\n",
            "[622 | 1536.51] loss=1.68 avg=2.06\n",
            "[623 | 1538.87] loss=2.39 avg=2.06\n",
            "[624 | 1541.25] loss=1.33 avg=2.06\n",
            "[625 | 1543.62] loss=1.43 avg=2.05\n",
            "[626 | 1545.99] loss=1.72 avg=2.05\n",
            "[627 | 1548.36] loss=1.86 avg=2.05\n",
            "[628 | 1550.72] loss=1.53 avg=2.04\n",
            "[629 | 1553.08] loss=2.36 avg=2.04\n",
            "[630 | 1555.45] loss=2.11 avg=2.04\n",
            "[631 | 1557.81] loss=1.67 avg=2.04\n",
            "[632 | 1560.17] loss=1.78 avg=2.04\n",
            "[633 | 1562.54] loss=1.61 avg=2.03\n",
            "[634 | 1564.90] loss=1.62 avg=2.03\n",
            "[635 | 1567.28] loss=1.74 avg=2.03\n",
            "[636 | 1569.65] loss=1.38 avg=2.02\n",
            "[637 | 1572.01] loss=1.67 avg=2.02\n",
            "[638 | 1574.38] loss=1.79 avg=2.01\n",
            "[639 | 1576.75] loss=1.60 avg=2.01\n",
            "[640 | 1579.12] loss=1.68 avg=2.01\n",
            "[641 | 1581.49] loss=1.47 avg=2.00\n",
            "[642 | 1583.86] loss=1.73 avg=2.00\n",
            "[643 | 1586.23] loss=1.78 avg=2.00\n",
            "[644 | 1588.61] loss=1.81 avg=1.99\n",
            "[645 | 1590.98] loss=1.63 avg=1.99\n",
            "[646 | 1593.36] loss=1.57 avg=1.99\n",
            "[647 | 1595.73] loss=1.48 avg=1.98\n",
            "[648 | 1598.11] loss=1.84 avg=1.98\n",
            "[649 | 1600.48] loss=1.44 avg=1.98\n",
            "[650 | 1602.86] loss=1.26 avg=1.97\n",
            "[651 | 1605.24] loss=1.88 avg=1.97\n",
            "[652 | 1607.61] loss=1.92 avg=1.97\n",
            "[653 | 1609.98] loss=1.55 avg=1.96\n",
            "[654 | 1612.36] loss=1.86 avg=1.96\n",
            "[655 | 1614.73] loss=1.21 avg=1.95\n",
            "[656 | 1617.11] loss=1.52 avg=1.95\n",
            "[657 | 1619.49] loss=1.71 avg=1.95\n",
            "[658 | 1621.87] loss=1.63 avg=1.94\n",
            "[659 | 1624.24] loss=1.22 avg=1.94\n",
            "[660 | 1626.61] loss=1.71 avg=1.93\n",
            "[661 | 1628.99] loss=2.05 avg=1.94\n",
            "[662 | 1631.37] loss=1.67 avg=1.93\n",
            "[663 | 1633.75] loss=1.44 avg=1.93\n",
            "[664 | 1636.12] loss=1.66 avg=1.93\n",
            "[665 | 1638.50] loss=1.36 avg=1.92\n",
            "[666 | 1640.87] loss=1.98 avg=1.92\n",
            "[667 | 1643.25] loss=1.70 avg=1.92\n",
            "[668 | 1645.63] loss=2.04 avg=1.92\n",
            "[669 | 1648.01] loss=1.61 avg=1.92\n",
            "[670 | 1650.37] loss=1.68 avg=1.91\n",
            "[671 | 1652.75] loss=2.03 avg=1.91\n",
            "[672 | 1655.14] loss=1.38 avg=1.91\n",
            "[673 | 1657.52] loss=1.79 avg=1.91\n",
            "[674 | 1659.89] loss=1.80 avg=1.91\n",
            "[675 | 1662.28] loss=1.66 avg=1.90\n",
            "[676 | 1664.67] loss=1.29 avg=1.90\n",
            "[677 | 1667.05] loss=1.93 avg=1.90\n",
            "[678 | 1669.42] loss=1.49 avg=1.89\n",
            "[679 | 1671.80] loss=1.91 avg=1.90\n",
            "[680 | 1674.19] loss=1.50 avg=1.89\n",
            "[681 | 1676.56] loss=1.64 avg=1.89\n",
            "[682 | 1678.93] loss=1.98 avg=1.89\n",
            "[683 | 1681.31] loss=1.26 avg=1.88\n",
            "[684 | 1683.70] loss=2.11 avg=1.89\n",
            "[685 | 1686.07] loss=1.37 avg=1.88\n",
            "[686 | 1688.45] loss=1.73 avg=1.88\n",
            "[687 | 1690.82] loss=1.86 avg=1.88\n",
            "[688 | 1693.19] loss=1.29 avg=1.87\n",
            "[689 | 1695.58] loss=2.27 avg=1.88\n",
            "[690 | 1697.95] loss=1.64 avg=1.87\n",
            "[691 | 1700.33] loss=2.00 avg=1.88\n",
            "[692 | 1702.71] loss=1.55 avg=1.87\n",
            "[693 | 1705.09] loss=1.78 avg=1.87\n",
            "[694 | 1707.46] loss=1.74 avg=1.87\n",
            "[695 | 1709.85] loss=1.89 avg=1.87\n",
            "[696 | 1712.23] loss=1.70 avg=1.87\n",
            "[697 | 1714.60] loss=1.83 avg=1.87\n",
            "[698 | 1716.98] loss=2.20 avg=1.87\n",
            "[699 | 1719.35] loss=1.92 avg=1.87\n",
            "[700 | 1721.72] loss=1.92 avg=1.87\n",
            "======== SAMPLE 1 ========\n",
            "BER and so forth i m not sure if this has anything to do with the fact that they don t need to sell the license or that the media tends to ignore everything that happens around them \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url URL date not supplied o uksun is the latest in a family of startups aiming to bring the NUMBER bmc to the masses \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url URL date not supplied a new edition of the fop quiz showed that people with opposites minds were the same as people with opposites brains what does this have to do with the similarity of children s minds then the researchers looked at the brains of the same people i was talking to at first i thought i wasn t even going to look at the kids and what i saw was utter chaos i think i poked that thought out of my mind for a couple minutes until i found out who is who and what does it say in the book that people with opposites minds are equal to people with opposites brains and thus the fop quiz had to be changed as the fop quiz shows the brain test results changed by the editors of the journal of neuroanatomy changes as the year NUMBER starts and the fop test results changed by the editors of the journal of neuroanatomy changes as the number of people with different minds increases the experimenters start to get the best of both worlds the fop test results show that the people with opposites minds get the most from the people with opposites brains so it seems like there would have to be some relation there one group gets more sympathy the other gets the logical conclusion that has no relation at all the fop test results are really quite telling the fop results in this study are not correlational they are not causal probabilists so they are asking the question that every experiment needs to answer the correct answer to a question like when does the world stop spinning out of nothing it must have been wonderful to be able to say the same thing to someone maybe that song has been on my playlist for a few hours now but it can t be meaningful i d also draw this conclusion without the additional context you need to know what you are wondering yourself btw it seems that the people who appear to be the most effective at what they do with the time are those children who don t understand that the only answer given is the one they wish they were born in it is a completely irrelevant distraction from the real issue here is that the media has a conflict of interest as both companies and individuals claim and that of course a conflict of interest also exists between a startup and people who work for them and in particular it s a conflict of interests between the interests of the individuals who are writing the checks and the interests of the journals and papers the research has a conflict of interests in that part of the world it would seem that an outsider with a stake in the decision making process would fall into this category the fop award winning study that you just read is the work of someone who has no conflict of interest and who has no relevant relevant information i don t find this relevant it appears to me that the conflict parenthood category should not have these special circumstances and i m not convinced that this study really qualifies as such because the relevant relevant relevant information you receive in writing an article such as this doesn t even begin to answer your question does this conflict really exist and if so why should people be motivated to report on this issue anyway that is contrary to our best interests when are neale nitro as again the relevance of this study in your point of view is it really relevant not to neale nitro but to your own personal beliefs and worldview i really felt it was important to ask neale nitro this question the short answer is we want to know the political figures responsible for creating the conditions that led to the current havoc we want specific records of how they fared at that moment and the political figures responsible for creating the conditions during the last major crisis who are responsible for creating that mess obviously there are individuals at the top responsible for creating that mess but the political figures responsible for creating the conditions during the last major crisis all had responsibility for creating that mess and we all want to hear the full story but here we ve got it all backwards these are the same people who claim political figures are illegitimate because of their political figures that distort the facts to the detriment of the group representing and their financial interests this is the same business people who want you to believe we re all in it for the good guys in this day and age where everyone has a voice so let us make sure that no one gets in the way so that nobody gets in the way again when we re going to have regulations that will make it difficult for some people to maintain that position if they get caught saying the words or ouch ouch don t get caught it s an unfortunate consequence of the political system but i find it particularly saddening that the media would want to make the distinction between the interests of the political parties who have overlapping interests and those who merely hold large sums of money and interests both parties can t really be in the same camp when the\n",
            "\n",
            "[701 | 1733.24] loss=1.37 avg=1.87\n",
            "[702 | 1735.61] loss=1.16 avg=1.86\n",
            "[703 | 1737.98] loss=1.14 avg=1.85\n",
            "[704 | 1740.35] loss=1.83 avg=1.85\n",
            "[705 | 1742.72] loss=2.00 avg=1.85\n",
            "[706 | 1745.10] loss=1.52 avg=1.85\n",
            "[707 | 1747.47] loss=1.28 avg=1.85\n",
            "[708 | 1749.84] loss=1.63 avg=1.84\n",
            "[709 | 1752.20] loss=2.19 avg=1.85\n",
            "[710 | 1754.58] loss=1.58 avg=1.84\n",
            "[711 | 1756.95] loss=2.15 avg=1.85\n",
            "[712 | 1759.31] loss=1.98 avg=1.85\n",
            "[713 | 1761.68] loss=1.79 avg=1.85\n",
            "[714 | 1764.06] loss=1.48 avg=1.84\n",
            "[715 | 1766.43] loss=1.96 avg=1.85\n",
            "[716 | 1768.80] loss=1.40 avg=1.84\n",
            "[717 | 1771.17] loss=1.62 avg=1.84\n",
            "[718 | 1773.53] loss=1.08 avg=1.83\n",
            "[719 | 1775.90] loss=1.11 avg=1.82\n",
            "[720 | 1778.27] loss=1.52 avg=1.82\n",
            "[721 | 1780.63] loss=1.90 avg=1.82\n",
            "[722 | 1783.01] loss=1.90 avg=1.82\n",
            "[723 | 1785.38] loss=1.82 avg=1.82\n",
            "[724 | 1787.75] loss=1.87 avg=1.82\n",
            "[725 | 1790.12] loss=1.57 avg=1.82\n",
            "[726 | 1792.49] loss=1.69 avg=1.82\n",
            "[727 | 1794.87] loss=1.04 avg=1.81\n",
            "[728 | 1797.24] loss=1.43 avg=1.81\n",
            "[729 | 1799.61] loss=1.53 avg=1.80\n",
            "[730 | 1801.97] loss=1.75 avg=1.80\n",
            "[731 | 1804.33] loss=1.36 avg=1.80\n",
            "[732 | 1806.69] loss=1.04 avg=1.79\n",
            "[733 | 1809.06] loss=2.08 avg=1.79\n",
            "[734 | 1811.42] loss=1.55 avg=1.79\n",
            "[735 | 1813.80] loss=1.75 avg=1.79\n",
            "[736 | 1816.15] loss=0.78 avg=1.78\n",
            "[737 | 1818.53] loss=1.73 avg=1.78\n",
            "[738 | 1820.89] loss=1.69 avg=1.78\n",
            "[739 | 1823.25] loss=2.03 avg=1.78\n",
            "[740 | 1825.62] loss=1.61 avg=1.78\n",
            "[741 | 1827.99] loss=1.57 avg=1.78\n",
            "[742 | 1830.37] loss=1.48 avg=1.78\n",
            "[743 | 1832.75] loss=2.20 avg=1.78\n",
            "[744 | 1835.12] loss=1.64 avg=1.78\n",
            "[745 | 1837.49] loss=1.66 avg=1.78\n",
            "[746 | 1839.86] loss=1.15 avg=1.77\n",
            "[747 | 1842.21] loss=0.95 avg=1.76\n",
            "[748 | 1844.58] loss=1.48 avg=1.76\n",
            "[749 | 1846.95] loss=1.59 avg=1.76\n",
            "[750 | 1849.31] loss=1.31 avg=1.75\n",
            "[751 | 1851.68] loss=1.59 avg=1.75\n",
            "[752 | 1854.05] loss=1.30 avg=1.75\n",
            "[753 | 1856.42] loss=1.56 avg=1.75\n",
            "[754 | 1858.79] loss=1.79 avg=1.75\n",
            "[755 | 1861.15] loss=1.54 avg=1.74\n",
            "[756 | 1863.52] loss=1.28 avg=1.74\n",
            "[757 | 1865.90] loss=1.80 avg=1.74\n",
            "[758 | 1868.28] loss=1.53 avg=1.74\n",
            "[759 | 1870.64] loss=1.71 avg=1.74\n",
            "[760 | 1873.01] loss=1.05 avg=1.73\n",
            "[761 | 1875.38] loss=0.84 avg=1.72\n",
            "[762 | 1877.75] loss=1.58 avg=1.72\n",
            "[763 | 1880.12] loss=1.51 avg=1.72\n",
            "[764 | 1882.48] loss=1.88 avg=1.72\n",
            "[765 | 1884.84] loss=1.19 avg=1.72\n",
            "[766 | 1887.21] loss=1.38 avg=1.71\n",
            "[767 | 1889.58] loss=1.60 avg=1.71\n",
            "[768 | 1891.95] loss=0.75 avg=1.70\n",
            "[769 | 1894.32] loss=1.14 avg=1.70\n",
            "[770 | 1896.69] loss=1.41 avg=1.69\n",
            "[771 | 1899.06] loss=1.39 avg=1.69\n",
            "[772 | 1901.44] loss=1.25 avg=1.69\n",
            "[773 | 1903.81] loss=1.45 avg=1.68\n",
            "[774 | 1906.18] loss=1.15 avg=1.68\n",
            "[775 | 1908.54] loss=1.63 avg=1.68\n",
            "[776 | 1910.92] loss=1.16 avg=1.67\n",
            "[777 | 1913.28] loss=1.60 avg=1.67\n",
            "[778 | 1915.64] loss=1.29 avg=1.67\n",
            "[779 | 1918.02] loss=1.28 avg=1.66\n",
            "[780 | 1920.40] loss=1.75 avg=1.66\n",
            "[781 | 1922.77] loss=1.56 avg=1.66\n",
            "[782 | 1925.13] loss=1.34 avg=1.66\n",
            "[783 | 1927.50] loss=1.50 avg=1.66\n",
            "[784 | 1929.87] loss=1.15 avg=1.65\n",
            "[785 | 1932.24] loss=1.42 avg=1.65\n",
            "[786 | 1934.60] loss=2.06 avg=1.66\n",
            "[787 | 1936.97] loss=1.53 avg=1.65\n",
            "[788 | 1939.35] loss=1.14 avg=1.65\n",
            "[789 | 1941.71] loss=1.71 avg=1.65\n",
            "[790 | 1944.08] loss=1.82 avg=1.65\n",
            "[791 | 1946.46] loss=0.85 avg=1.64\n",
            "[792 | 1948.83] loss=1.56 avg=1.64\n",
            "[793 | 1951.21] loss=0.77 avg=1.63\n",
            "[794 | 1953.58] loss=1.70 avg=1.63\n",
            "[795 | 1955.95] loss=1.15 avg=1.63\n",
            "[796 | 1958.31] loss=1.01 avg=1.62\n",
            "[797 | 1960.69] loss=1.72 avg=1.62\n",
            "[798 | 1963.06] loss=1.49 avg=1.62\n",
            "[799 | 1965.43] loss=0.95 avg=1.62\n",
            "[800 | 1967.80] loss=1.86 avg=1.62\n",
            "======== SAMPLE 1 ========\n",
            " users of this product to whom i can add the below information if they are not explicitly stated they must be broken the product may contain unverified information i have no doubt that the device is a pNUMBERjNUMBER device it was manufactured at the p NUMBERb which has all the features of a pNUMBERjNUMBER it is shipped with a serial number which i am unable to provide you the serial number of the pNUMBERfNUMBER it was assembled using NUMBERdtech NUMBER pNUMBERcNUMBER jesse kehoe \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url URL date NUMBER NUMBER NUMBERtNUMBER NUMBER NUMBER NUMBER NUMBER i am thinking about revamping my email program today when i log on i am more motivated to stop sending email instead i set my alarm to hang an error message telling you how to reset it to an empty state but when it does get reset to zero i know i should have turned off alarm clock by then and the alarm clock was already giving way at some point and it appeared to be doing its thing so if there was a way to reset the alarm clock it must have been useful nonetheless after over a year or so without the feature set i finally got around the habit and simply have the alarm clock display as it should be and save it as something to keep on your hard disk for later use i also take alarm clock notes to keep on a hard disk when i get home every morning all i ever take with me is the cramp that is on your finger and at some point you will have figured out what you want to keep on your hard disk the best thing about being a tiny microcomputer is that almost any program program can run on it however you have virtual desktops and other virtual desktops only one machine is truly visible to other virtual machines all other pages of disk images are read by disk rick garza rick laban masnick at the university of wales web services a university in south wales germany URL this URL email is sponsored by thinkgeek welcome to geek heaven URL _______________________________________________ spamassassin commits mailing list spamassassin commits URL URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url URL date not supplied but perhaps you have a nice to look forward to list of goodies from the listists we have several more you can find at the other end of the spectrum i do not want to give up anything and would like to share what i have had with this subject we d see how spamassassin performs against other load balancers when testing multiple traffic models from spamassassin and perhaps some of the spamc cdb tools i know some developers and testers who are good at parsing xml do a very fine job of parsing html do a very fine job of parsing java do a very fine job of parsing xml and so on it gets a little muddied a bit but still a good representation of what s going on here and with a few additions we are seeing a very good pipeline from the NUMBERk to NUMBERk messages spambayes are usually much slower than html in fact i m not sure how that s possible although i d say it s not b if spamassassin is throwing out uninteresting messages more frequently than html it almost feels like they were unwrapped messages were not misclassified they re indeed simply not throwing them out correctly james tauber s news service URL URL is now available from URL for download this article from URL URL URL by URL NUMBER URL NUMBER NUMBER NUMBER NUMBER NUMBER it s NUMBER URL \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "url URL date NUMBER NUMBER NUMBERtNUMBER NUMBER NUMBER NUMBER NUMBER at NUMBER NUMBER justin mason writes about gory religions he grew up believing that the physical features of the human form can be used to justify godhood everywhere he went he was raised around this view and in for a horror story he had his temple at chapman in charlton near gloucesters bl court inns and castle in leecher s grotto and he decided to take it out himself it was the only one of his that i could spot exactly where the treasure was it really was priceless anyway the treasure came as a plastic bag filled with a hot glue gun and a couple of scissors i took it to a neighbour s and he proceeded to sed him with the hot glue gun and the scissors were ready when the priest arrived with his team of geologists he found a vast body of rock stuck together with bone and ash suggesting that it was some kind of meteorite striking the earth some geologists have suggested it to be almost a bit bigger than the earth itself geologists have even dubbed it the earth of june NUMBER NUMBER i m told that the temperature was NUMBER degrees celsius NUMBER the size of india and with a hint of green hell bent onto it that s a far cry from smelly cool indeed smelly but not cool as smelly as greys it wasn t green it was rather muddied by the fact that it had been\n",
            "\n",
            "[801 | 1979.33] loss=0.81 avg=1.61\n",
            "[802 | 1981.69] loss=1.52 avg=1.61\n",
            "[803 | 1984.07] loss=1.26 avg=1.61\n",
            "[804 | 1986.43] loss=1.43 avg=1.60\n",
            "[805 | 1988.80] loss=1.00 avg=1.60\n",
            "[806 | 1991.19] loss=1.29 avg=1.60\n",
            "[807 | 1993.56] loss=0.99 avg=1.59\n",
            "[808 | 1995.92] loss=1.41 avg=1.59\n",
            "[809 | 1998.28] loss=1.31 avg=1.58\n",
            "[810 | 2000.65] loss=0.92 avg=1.58\n",
            "[811 | 2003.03] loss=1.75 avg=1.58\n",
            "[812 | 2005.40] loss=1.58 avg=1.58\n",
            "[813 | 2007.77] loss=1.60 avg=1.58\n",
            "[814 | 2010.14] loss=1.41 avg=1.58\n",
            "[815 | 2012.50] loss=1.34 avg=1.58\n",
            "[816 | 2014.87] loss=0.84 avg=1.57\n",
            "[817 | 2017.23] loss=1.20 avg=1.56\n",
            "[818 | 2019.61] loss=1.68 avg=1.57\n",
            "[819 | 2021.98] loss=1.31 avg=1.56\n",
            "[820 | 2024.34] loss=1.62 avg=1.56\n",
            "[821 | 2026.70] loss=1.72 avg=1.57\n",
            "[822 | 2029.07] loss=0.88 avg=1.56\n",
            "[823 | 2031.44] loss=0.91 avg=1.55\n",
            "[824 | 2033.80] loss=1.25 avg=1.55\n",
            "[825 | 2036.18] loss=1.19 avg=1.55\n",
            "[826 | 2038.54] loss=1.31 avg=1.54\n",
            "[827 | 2040.91] loss=1.45 avg=1.54\n",
            "[828 | 2043.27] loss=1.10 avg=1.54\n",
            "[829 | 2045.65] loss=1.17 avg=1.53\n",
            "[830 | 2048.01] loss=1.25 avg=1.53\n",
            "[831 | 2050.38] loss=1.11 avg=1.53\n",
            "[832 | 2052.75] loss=1.14 avg=1.52\n",
            "[833 | 2055.11] loss=2.03 avg=1.53\n",
            "[834 | 2057.49] loss=1.22 avg=1.53\n",
            "[835 | 2059.85] loss=1.16 avg=1.52\n",
            "[836 | 2062.22] loss=0.75 avg=1.51\n",
            "[837 | 2064.58] loss=1.12 avg=1.51\n",
            "[838 | 2066.95] loss=1.42 avg=1.51\n",
            "[839 | 2069.32] loss=1.09 avg=1.50\n",
            "[840 | 2071.70] loss=1.41 avg=1.50\n",
            "[841 | 2074.06] loss=1.22 avg=1.50\n",
            "[842 | 2076.43] loss=1.18 avg=1.50\n",
            "[843 | 2078.80] loss=1.20 avg=1.49\n",
            "[844 | 2081.17] loss=1.84 avg=1.50\n",
            "[845 | 2083.54] loss=1.48 avg=1.50\n",
            "[846 | 2085.92] loss=1.05 avg=1.49\n",
            "[847 | 2088.29] loss=0.98 avg=1.49\n",
            "[848 | 2090.67] loss=1.48 avg=1.49\n",
            "[849 | 2093.03] loss=0.96 avg=1.48\n",
            "[850 | 2095.41] loss=1.27 avg=1.48\n",
            "[851 | 2097.78] loss=0.96 avg=1.48\n",
            "[852 | 2100.16] loss=1.45 avg=1.48\n",
            "[853 | 2102.53] loss=1.75 avg=1.48\n",
            "[854 | 2104.91] loss=1.28 avg=1.48\n",
            "[855 | 2107.28] loss=1.13 avg=1.47\n",
            "[856 | 2109.64] loss=1.23 avg=1.47\n",
            "[857 | 2112.01] loss=1.39 avg=1.47\n",
            "[858 | 2114.37] loss=1.31 avg=1.47\n",
            "[859 | 2116.73] loss=1.55 avg=1.47\n",
            "[860 | 2119.10] loss=1.15 avg=1.47\n",
            "[861 | 2121.48] loss=1.36 avg=1.46\n",
            "[862 | 2123.86] loss=1.18 avg=1.46\n",
            "[863 | 2126.22] loss=1.44 avg=1.46\n",
            "[864 | 2128.59] loss=1.96 avg=1.47\n",
            "[865 | 2130.96] loss=1.22 avg=1.46\n",
            "[866 | 2133.33] loss=1.12 avg=1.46\n",
            "[867 | 2135.70] loss=1.43 avg=1.46\n",
            "[868 | 2138.07] loss=1.15 avg=1.46\n",
            "[869 | 2140.45] loss=1.21 avg=1.45\n",
            "[870 | 2142.82] loss=1.23 avg=1.45\n",
            "[871 | 2145.20] loss=1.91 avg=1.46\n",
            "[872 | 2147.57] loss=0.88 avg=1.45\n",
            "[873 | 2149.94] loss=0.69 avg=1.44\n",
            "[874 | 2152.31] loss=0.99 avg=1.44\n",
            "[875 | 2154.68] loss=0.84 avg=1.43\n",
            "[876 | 2157.05] loss=1.53 avg=1.43\n",
            "[877 | 2159.41] loss=1.03 avg=1.43\n",
            "[878 | 2161.79] loss=1.34 avg=1.43\n",
            "[879 | 2164.16] loss=1.32 avg=1.43\n",
            "[880 | 2166.54] loss=1.56 avg=1.43\n",
            "[881 | 2168.90] loss=1.10 avg=1.43\n",
            "[882 | 2171.27] loss=1.00 avg=1.42\n",
            "[883 | 2173.64] loss=1.61 avg=1.42\n",
            "[884 | 2176.02] loss=1.35 avg=1.42\n",
            "[885 | 2178.40] loss=1.44 avg=1.42\n",
            "[886 | 2180.77] loss=1.67 avg=1.43\n",
            "[887 | 2183.12] loss=1.07 avg=1.42\n",
            "[888 | 2185.49] loss=1.16 avg=1.42\n",
            "[889 | 2187.86] loss=1.42 avg=1.42\n",
            "[890 | 2190.23] loss=0.92 avg=1.41\n",
            "[891 | 2192.60] loss=1.41 avg=1.41\n",
            "[892 | 2194.97] loss=1.33 avg=1.41\n",
            "[893 | 2197.34] loss=1.27 avg=1.41\n",
            "[894 | 2199.71] loss=0.97 avg=1.41\n",
            "[895 | 2202.08] loss=0.84 avg=1.40\n",
            "[896 | 2204.45] loss=0.99 avg=1.40\n",
            "[897 | 2206.82] loss=1.09 avg=1.39\n",
            "[898 | 2209.18] loss=1.32 avg=1.39\n",
            "[899 | 2211.54] loss=1.07 avg=1.39\n",
            "[900 | 2213.91] loss=1.36 avg=1.39\n",
            "======== SAMPLE 1 ========\n",
            " news of course is that i need a place to live and work i think that s a city that wants a country that everyone else wants everywhere i m not suggesting i can just lie and think i don t live in a nice apartment block or something along that street if someone came along and said we had to have a fence or a fence on our street and had to have a pipeline come to us from somewhere they d probably get pissed and walk up and say we have to have a pipeline the people who live in those buildings don t have a fence and have no knowledge of how to do this kind of planning come a few blocks away from them are people who are smart enough to figure this stuff out themselves if deterrence worked there would be plenty of people in that building already right now to keep it from getting too expensive for most people that won t stop them or make it more expensive for everyone else that s what deterrence was designed for i m not opposed to a long term solution to this especially with the exception of a few short term options but we have a huge tax increase on the whole which will be hard to swallow for most people unless they really are willing to live in a city which doesn t care about their future or if they do anyway NUMBER different types of development is very much the case in downtown core what does the unique situation of having multiple transit systems mean in a mix of different suburban areas mean to you it means that businesses not just residents would have access to buses and could walk to and from work if they wanted one another it means that the transit system as a whole would grow exponentially with each new person coming along so that everyone is on a shared path NUMBER different technologies are being developed to deal with congestion and traffic calming techniques to manage it are being developed in coordination with other agencies such as transit improvements managers and transit coordination officers this approach will provide better coordination between agencies and reduce unnecessary delays in starting to road walk a system in central business district during rush hour there is a lot of thought involved in planning and developing a transit system in order to get people into the city regardless of when they leave this is a highly emotional issue and we tend to be very protective of our bridges and tunnels we don t want people coming through us with our transit system in place we want people coming in from other cities to be able to walk or take a bus our primary method of transit is through a shuttle bus a system in downtown core the NUMBER mile stretch where apartment buildings dot the block and the block are having a period of historic decline the exact same buildings which are now being torn down along with commercial space along with the transit center itself was once part of the core and is no longer part of the central business district that s a real tragedy for those of us on public transportation throughout the NUMBER s a council may be responsible for approving or rejecting development projects in downtown core as the effects of these impacts are felt all the time in small groups and on a very personal level by the individuals affected the NUMBER is a very personal issue for me because of the emotions involved many of those affected have feelings of isolation and that impacts both personal and professional as well as for me personally i think my emotional pain and perspective on the neighborhood are very candid and this neighborhood may be the most vulnerable when it comes to crisis but i also think that many of the people present are not equipped to deal with that specifically but are thinking about it in a very personal and non inflammatory way and it s very personal and it s very personal for many of the people living here today that emotional discomfort may not be as widespread as it was personally i think what has happened in this situation and what has happened in this core disruption that is very personal for many of the individuals involved have affected me personally and i don t want my name or information to be used address my mail for this reason i don t want people to think that i m insensitive on this issue because i think what i have said is very personal in a very personal way a few days before our meeting one of the people who attended our first financial institutions meeting was able to talk herself out of a store because of my personal experiences first hand she said what a load of crap we are dealing with mentally ill people emotionally ill people that we should be treating with the utmost compassion and care that we have dealt with before but what a load of crap is the world really a mental institution with all the trappings and all the sanctities that must be followed i attended the first public hearings for new york public works and construction here for undergrad my wife had a breakdown which was the result of a breakdown mail theft and homicide problem we lived in a very depressed area until we moved here in the NUMBER s we don t have a shortage of jobs now there are almost a million single mother families with NUMBER or NUMBER year old children living in poverty our rent for a single room is NUMBER NUMBER dollars a week our utility bill is just NUMBER NUMBER dollars a year living in poverty is a real struggle but every single parent in our group has made a living cleaning up and relieving ourselves to do household chores since\n",
            "\n",
            "[901 | 2225.46] loss=1.13 avg=1.39\n",
            "[902 | 2227.83] loss=1.32 avg=1.39\n",
            "[903 | 2230.21] loss=0.94 avg=1.38\n",
            "[904 | 2232.58] loss=1.22 avg=1.38\n",
            "[905 | 2234.95] loss=1.18 avg=1.38\n",
            "[906 | 2237.31] loss=1.20 avg=1.38\n",
            "[907 | 2239.67] loss=1.39 avg=1.38\n",
            "[908 | 2242.04] loss=0.85 avg=1.37\n",
            "[909 | 2244.41] loss=1.02 avg=1.37\n",
            "[910 | 2246.78] loss=1.11 avg=1.37\n",
            "[911 | 2249.16] loss=1.27 avg=1.37\n",
            "[912 | 2251.52] loss=1.02 avg=1.36\n",
            "[913 | 2253.90] loss=1.18 avg=1.36\n",
            "[914 | 2256.26] loss=1.02 avg=1.36\n",
            "[915 | 2258.64] loss=0.92 avg=1.35\n",
            "[916 | 2261.00] loss=1.16 avg=1.35\n",
            "[917 | 2263.36] loss=1.25 avg=1.35\n",
            "[918 | 2265.73] loss=1.30 avg=1.35\n",
            "[919 | 2268.10] loss=0.84 avg=1.34\n",
            "[920 | 2270.48] loss=1.51 avg=1.35\n",
            "[921 | 2272.83] loss=1.26 avg=1.34\n",
            "[922 | 2275.20] loss=1.05 avg=1.34\n",
            "[923 | 2277.56] loss=1.33 avg=1.34\n",
            "[924 | 2279.93] loss=1.87 avg=1.35\n",
            "[925 | 2282.30] loss=0.86 avg=1.34\n",
            "[926 | 2284.67] loss=1.17 avg=1.34\n",
            "[927 | 2287.04] loss=1.00 avg=1.34\n",
            "[928 | 2289.42] loss=0.87 avg=1.33\n",
            "[929 | 2291.77] loss=1.01 avg=1.33\n",
            "[930 | 2294.14] loss=1.47 avg=1.33\n",
            "[931 | 2296.51] loss=0.91 avg=1.33\n",
            "[932 | 2298.89] loss=0.71 avg=1.32\n",
            "[933 | 2301.26] loss=1.15 avg=1.32\n",
            "[934 | 2303.62] loss=1.07 avg=1.32\n",
            "[935 | 2305.99] loss=1.28 avg=1.32\n",
            "[936 | 2308.37] loss=1.00 avg=1.31\n",
            "[937 | 2310.74] loss=1.16 avg=1.31\n",
            "[938 | 2313.11] loss=1.22 avg=1.31\n",
            "[939 | 2315.48] loss=1.06 avg=1.31\n",
            "[940 | 2317.85] loss=1.11 avg=1.30\n",
            "[941 | 2320.21] loss=0.64 avg=1.30\n",
            "[942 | 2322.59] loss=1.23 avg=1.30\n",
            "[943 | 2324.95] loss=0.97 avg=1.29\n",
            "[944 | 2327.31] loss=0.77 avg=1.29\n",
            "[945 | 2329.68] loss=1.12 avg=1.29\n",
            "[946 | 2332.06] loss=0.85 avg=1.28\n",
            "[947 | 2334.43] loss=0.96 avg=1.28\n",
            "[948 | 2336.81] loss=1.29 avg=1.28\n",
            "[949 | 2339.18] loss=1.34 avg=1.28\n",
            "[950 | 2341.55] loss=0.71 avg=1.27\n",
            "[951 | 2343.92] loss=1.38 avg=1.28\n",
            "[952 | 2346.30] loss=0.88 avg=1.27\n",
            "[953 | 2348.67] loss=1.08 avg=1.27\n",
            "[954 | 2351.03] loss=0.87 avg=1.27\n",
            "[955 | 2353.39] loss=1.14 avg=1.26\n",
            "[956 | 2355.77] loss=1.33 avg=1.27\n",
            "[957 | 2358.14] loss=0.89 avg=1.26\n",
            "[958 | 2360.51] loss=0.70 avg=1.26\n",
            "[959 | 2362.89] loss=1.51 avg=1.26\n",
            "[960 | 2365.25] loss=1.20 avg=1.26\n",
            "[961 | 2367.63] loss=0.80 avg=1.25\n",
            "[962 | 2370.00] loss=0.96 avg=1.25\n",
            "[963 | 2372.37] loss=1.32 avg=1.25\n",
            "[964 | 2374.74] loss=1.07 avg=1.25\n",
            "[965 | 2377.12] loss=0.92 avg=1.25\n",
            "[966 | 2379.49] loss=1.07 avg=1.24\n",
            "[967 | 2381.87] loss=1.56 avg=1.25\n",
            "[968 | 2384.25] loss=0.78 avg=1.24\n",
            "[969 | 2386.62] loss=0.78 avg=1.24\n",
            "[970 | 2389.00] loss=1.50 avg=1.24\n",
            "[971 | 2391.37] loss=1.56 avg=1.24\n",
            "[972 | 2393.74] loss=1.26 avg=1.24\n",
            "[973 | 2396.10] loss=0.83 avg=1.24\n",
            "[974 | 2398.48] loss=0.97 avg=1.24\n",
            "[975 | 2400.86] loss=1.04 avg=1.24\n",
            "[976 | 2403.23] loss=1.21 avg=1.24\n",
            "[977 | 2405.59] loss=1.19 avg=1.23\n",
            "[978 | 2407.97] loss=1.13 avg=1.23\n",
            "[979 | 2410.34] loss=0.84 avg=1.23\n",
            "[980 | 2412.71] loss=0.91 avg=1.23\n",
            "[981 | 2415.09] loss=0.81 avg=1.22\n",
            "[982 | 2417.46] loss=0.80 avg=1.22\n",
            "[983 | 2419.83] loss=0.86 avg=1.21\n",
            "[984 | 2422.19] loss=1.10 avg=1.21\n",
            "[985 | 2424.56] loss=1.17 avg=1.21\n",
            "[986 | 2426.92] loss=0.92 avg=1.21\n",
            "[987 | 2429.30] loss=0.59 avg=1.20\n",
            "[988 | 2431.67] loss=0.62 avg=1.20\n",
            "[989 | 2434.05] loss=0.95 avg=1.20\n",
            "[990 | 2436.42] loss=1.16 avg=1.20\n",
            "[991 | 2438.80] loss=0.99 avg=1.19\n",
            "[992 | 2441.18] loss=0.66 avg=1.19\n",
            "[993 | 2443.55] loss=0.96 avg=1.19\n",
            "[994 | 2445.92] loss=0.78 avg=1.18\n",
            "[995 | 2448.28] loss=1.61 avg=1.19\n",
            "[996 | 2450.64] loss=0.85 avg=1.18\n",
            "[997 | 2453.01] loss=0.73 avg=1.18\n",
            "[998 | 2455.38] loss=0.93 avg=1.18\n",
            "[999 | 2457.76] loss=0.76 avg=1.17\n",
            "[1000 | 2460.13] loss=0.86 avg=1.17\n",
            "Saving checkpoint/run1/model-1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4ee0bbdc11d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m               steps=1000)   # steps is max number of training steps\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mload_gpt2\u001b[0;34m(sess, checkpoint, run_name, checkpoint_dir, model_name, model_dir, multi_gpu)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'latest'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 183\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    184\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    185\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/tensorflow-1.15.2/python3.7/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQumnDPkVqLs"
      },
      "source": [
        "with open('generated_ham.txt', 'w') as f:\n",
        "    for i in range(250):\n",
        "      text = gpt2.generate(session, return_as_list=True)[0]\n",
        "      f.write('%s\\n\\n\\n\\n\\n\\n\\n\\n\\n' % text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aurihBnXmCY1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}